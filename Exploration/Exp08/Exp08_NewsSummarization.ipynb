{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e78c969",
      "metadata": {
        "id": "5e78c969"
      },
      "source": [
        "# **뉴스기사 요약**\n",
        "---\n",
        "![이](https://github.com/riverlike/Aiffel_Project/blob/main/Exploration/Exp08/data/news.png?raw=true)\n",
        "\n",
        "이번 프로젝트에서는 뉴스의 내용을 학습해 주요내용(Headline)을 요약하는 모델을 구현해보겠다.\n",
        "뉴스 기사 데이터(news_summary_more.csv)는 인도의 뉴스 사이트인 Inshorts.com에서 스크래핑된 데이터셋이다. \n",
        " - [sunnysai12345/News_Summary](https://github.com/sunnysai12345/News_Summary)\n",
        "\n",
        "인도 뉴스이기때문에 인도식영어 문장이 있을수도 있으므로 테스트시 영국식영어 문장으로 구성된 텍스트를 위주로 분석을 진행해보겠다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "뉴스 기사 요약은 아래의 두가지 방법으로 진행하겠다.\n",
        "1. 추상적 요약(Abstractive Summarization) \n",
        " - seq2seq(sequence to sequence) 모델 이용\n",
        " - text를 본문, headlines를 이미 요약된 데이터로 삼아서 모델을 학습을 진행\n",
        "\n",
        "2. 추출적 요약(Extractive Summarization)\n",
        " - Summa의 summarize() 함수 활용\n",
        " - 오직 text열만을 사용하면 관련내용을 간단하게 요약해준다.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9e5afab8",
      "metadata": {
        "id": "9e5afab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23aa4cd4-2e63-490c-cb0a-ac710019b134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.33.135.242:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.33.135.242:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "#TPU설정\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "resolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리"
      ],
      "metadata": {
        "id": "3TBmizB9S9bX"
      },
      "id": "3TBmizB9S9bX"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n"
      ],
      "metadata": {
        "id": "hg5C3KgO7kyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774038ae-e3c7-42db-83aa-d2db7530f55c"
      },
      "id": "hg5C3KgO7kyT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 수집하기\n",
        "\n",
        "뉴스 기사 데이터(news_summary_more.csv)를 다운로드해보자.\n",
        "\n",
        "*  read_csv encoding\n",
        " - encoding 기본값 :  UTF when reading/writing (ex. ‘utf-8’)\n",
        " - encoding='iso-8859-1': Western Europe(latin_1 )"
      ],
      "metadata": {
        "id": "kNW0xoAVTART"
      },
      "id": "kNW0xoAVTART"
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "id": "83af479d",
      "metadata": {
        "id": "83af479d"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
        "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM3_g6yxZjGp",
        "outputId": "20854105-d39a-48de-921e-a9be6f4645ed"
      },
      "id": "JM3_g6yxZjGp",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98401, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f5683bb7",
      "metadata": {
        "id": "f5683bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ba6874a4-fbb4-45fd-d88e-caf7b119189f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fdde66a9-5523-45e1-b010-d7e1c7019908\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>98396</th>\n",
              "      <td>CRPF jawan axed to death by Maoists in Chhatti...</td>\n",
              "      <td>A CRPF jawan was on Tuesday axed to death with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98397</th>\n",
              "      <td>First song from Sonakshi Sinha's 'Noor' titled...</td>\n",
              "      <td>'Uff Yeh', the first song from the Sonakshi Si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98398</th>\n",
              "      <td>'The Matrix' film to get a reboot: Reports</td>\n",
              "      <td>According to reports, a new version of the 199...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98399</th>\n",
              "      <td>Snoop Dogg aims gun at clown dressed as Trump ...</td>\n",
              "      <td>A new music video shows rapper Snoop Dogg aimi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98400</th>\n",
              "      <td>Madhesi Morcha withdraws support to Nepalese g...</td>\n",
              "      <td>Madhesi Morcha, an alliance of seven political...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdde66a9-5523-45e1-b010-d7e1c7019908')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fdde66a9-5523-45e1-b010-d7e1c7019908 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fdde66a9-5523-45e1-b010-d7e1c7019908');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               headlines                                               text\n",
              "98396  CRPF jawan axed to death by Maoists in Chhatti...  A CRPF jawan was on Tuesday axed to death with...\n",
              "98397  First song from Sonakshi Sinha's 'Noor' titled...  'Uff Yeh', the first song from the Sonakshi Si...\n",
              "98398         'The Matrix' film to get a reboot: Reports  According to reports, a new version of the 199...\n",
              "98399  Snoop Dogg aims gun at clown dressed as Trump ...  A new music video shows rapper Snoop Dogg aimi...\n",
              "98400  Madhesi Morcha withdraws support to Nepalese g...  Madhesi Morcha, an alliance of seven political..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ls0EpELF66pZ",
        "outputId": "6efbf9be-fb03-40c4-e4e6-73f0638c1fa2"
      },
      "id": "ls0EpELF66pZ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['headlines'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F3l_Hbj6_br5",
        "outputId": "24a45268-78f0-47c6-ea41-a60e50dfbf43"
      },
      "id": "F3l_Hbj6_br5",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Delhi techie wins free food from Swiggy for one year on CRED'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55722754",
      "metadata": {
        "id": "55722754"
      },
      "source": [
        "데이터셋에는 9만8천여건의 샘플데이터가 있으며, 기사의 본문에 해당되는 text와 headlines 두 가지 열로 구성되어있다. text 열의 내용을 요약한 것이 headlines 열이라고 할 수 있다. \n",
        "\n",
        "seq2seq모델을 통해  text 시퀀스를 입력받아 headlines의 시퀀스를 예측하도록 학습을 진행할 것인데, 모델에 입력하기전 text 시퀀스를 전처리 해주자.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 추상적 요약(Abstractive Summarization)"
      ],
      "metadata": {
        "id": "3IJmeeKjc24s"
      },
      "id": "3IJmeeKjc24s"
    },
    {
      "cell_type": "markdown",
      "id": "b038e7e2",
      "metadata": {
        "id": "b038e7e2"
      },
      "source": [
        "## 1-1. 텍스트 데이터 전처리하기\n",
        "    1. 중복 샘플 제거\n",
        "    2. 결측치 샘플 제거\n",
        "    3. 텍스트 정규화\n",
        "    4. 불용어(stopwords) 제거\n",
        "    5. 문장의 최대 길이 정하기\n",
        "    6. 시작 토큰과 종료 토큰 추가하기\n",
        "    7. 훈련 데이터와 테스트 데이터를 분리\n",
        "    8. 단어사전(vocabulary) 만들기 및 정수 인코딩\n",
        "    9. 빈문장이 있는지 다시한번 더 확인\n",
        "    10. 문장의 길이 맞추기 (pad_sequences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-1. 중복 샘플 제거"
      ],
      "metadata": {
        "id": "oTecTS1sAJEX"
      },
      "id": "oTecTS1sAJEX"
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "id": "81a13455",
      "metadata": {
        "id": "81a13455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f72c7ae-3e4d-4393-ca97-1d762f582cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
            "headlines 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n"
          ]
        }
      ],
      "source": [
        "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
        "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "headlines의 경우 요약 데이터이므로 중복값이 존재할수 있지만 뉴스 내용에  해당하는 text 데이터는 중복이되면 안되므로 중복샘플인 제거해야한다."
      ],
      "metadata": {
        "id": "4dAkmg47BOlu"
      },
      "id": "4dAkmg47BOlu"
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(subset=['text'], inplace=True)\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at7xYW4VAFXw",
        "outputId": "a5e62ba9-c878-4273-e647-98ed5800ce8a"
      },
      "id": "at7xYW4VAFXw",
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플수 : 98360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-2. 결측치 샘플 제거\n",
        "데이터에 Null 값이 남아있는지 알아보고 있다면 해당 샘플도 함께 제거해야한다."
      ],
      "metadata": {
        "id": "aEPSiPlGBKmG"
      },
      "id": "aEPSiPlGBKmG"
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpIKxi_MAFIX",
        "outputId": "a6eb8203-8d55-4d80-e3bf-5f244f167cdc"
      },
      "id": "xpIKxi_MAFIX",
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "headlines    0\n",
              "text         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 남아있는 98360 샘플중 결측치가 있는것은 없다."
      ],
      "metadata": {
        "id": "0Nbos-PwCC6I"
      },
      "id": "0Nbos-PwCC6I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-3. 텍스트 정규화\n",
        "98360 샘플의 문장에 단어들에 대해 정규화를 해주어야한다. 단어가 똑같은 의미를 가지고 있는 경우 다른 단어로 처리되지 않도록 해주는 작업이다.\n",
        "텍스트 정규화를 위한 사전(dictionary)은 아래 사이트를 참고하여 구성하였다.\n",
        "\n",
        "- [정규화 사전 출처](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)"
      ],
      "metadata": {
        "id": "GFnAml42CJp3"
      },
      "id": "GFnAml42CJp3"
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "JJJPMiPoAE6o"
      },
      "id": "JJJPMiPoAE6o",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-4. 불용어(stopwords) 제거\n",
        "\n",
        "이제 불용어를 제거해주어야한다. 불용어는 텍스트에는 자주 등장하지만 문맥적으로 중요한 의미를 담고있지 않다. 자연어 처리시 중요단어로 인식이 되지 않도록 제거해주어야한다. NLTK에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거할 것이다.\n",
        "\n",
        "- NLTK(Natural Language Toolkit) \n",
        " - 영어 기호, 통계, 자연어 처리를 위한 라이브러리\n",
        " - 불용어 사전 : I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어를 미리 정의하고 있다."
      ],
      "metadata": {
        "id": "Kt9TIPjBDOAa"
      },
      "id": "Kt9TIPjBDOAa"
    },
    {
      "cell_type": "code",
      "source": [
        "print('불용어 개수 :', len(stopwords.words('english') )) # NLTK가 정의한 영어 불용어 리스트를 리턴\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgEodrusCfpW",
        "outputId": "e34421f2-ce2b-4960-b99c-a1aa1de71616"
      },
      "id": "kgEodrusCfpW",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에 정의된 정규화 사전과 불용어사전을 이용해 텍스트 전처리를 할 함수를 구현해보겠다. 이 함수에서는 text, headlines 문장을 입력받아 소문자로 만들고 뉴스에 포함된 html태그및 특수문자도 함께 제거한 후 최종적으로 문장의 토큰을 반환한다.\n",
        "\n",
        "단 headlines문장의 경우 문장 요약의 결과로 표시될때 불용어가 제거되면 부자연스럽기 때문에 불용어 제거 없이 토크나이징만 진행할 것이다."
      ],
      "metadata": {
        "id": "43sB8IIIGEVI"
      },
      "id": "43sB8IIIGEVI"
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수\n",
        "def preprocess_sentence(sentence, remove_stopwords=True):\n",
        "    sentence = sentence.lower() # 텍스트 소문자화\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
        "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
        "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
        "    \n",
        "    # 불용어 제거 (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
        "    # 불용어 미제거 (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "Nh5MZP0nCfnF"
      },
      "id": "Nh5MZP0nCfnF",
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
        "clean_text = []\n",
        "for s in data['text']:\n",
        "    clean_text.append(preprocess_sentence(s))\n",
        "\n",
        "# 전처리 후 출력\n",
        "print(\"text 전처리 후 결과: \", clean_text[:3])"
      ],
      "metadata": {
        "id": "9rXfiGQiCfka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f11d48-bdd3-4baf-ca65-e30c83dabc76"
      },
      "id": "9rXfiGQiCfka",
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text 전처리 후 결과:  ['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers', 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit', 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 clean_headlines 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
        "clean_headlines = []\n",
        "for s in data['headlines']:\n",
        "    clean_headlines.append(preprocess_sentence(s, False))\n",
        "\n",
        "print(\"headlines 전처리 후 결과: \", clean_headlines[:3])"
      ],
      "metadata": {
        "id": "LXXeMvvLCfgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78797f6-d82a-43e6-ed32-25301418fdb8"
      },
      "id": "LXXeMvvLCfgy",
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "headlines 전처리 후 결과:  ['upgrad learner switches to career in ml al with salary hike', 'delhi techie wins free food from swiggy for one year on cred', 'new zealand end rohit sharma led india match winning streak']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess_sentence 함수로 삭제된 단어가 많을 것이므로 빈문장이 있지 않은지 확인해보아야한다. "
      ],
      "metadata": {
        "id": "0AcGpeZhItmi"
      },
      "id": "0AcGpeZhItmi"
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = clean_text\n",
        "data['headlines'] = clean_headlines\n",
        "\n",
        "# 빈 값을 Null 값으로 변환\n",
        "data.replace('', np.nan, inplace=True)"
      ],
      "metadata": {
        "id": "yd0VHLqKCfd2"
      },
      "id": "yd0VHLqKCfd2",
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr0u3JZpCfad",
        "outputId": "da59c484-2118-480b-9ec0-40804b08231e"
      },
      "id": "Qr0u3JZpCfad",
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "headlines    0\n",
              "text         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈값을 가지는 샘플이 확인되지 않았다. "
      ],
      "metadata": {
        "id": "dmAxr2YhLylf"
      },
      "id": "dmAxr2YhLylf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-5. 문장의 최대 길이 정하기\n",
        "학습을 진행하기 위해서는 학습에 사용할 문장의 최대 길이를 결정하고, 문장의 시작과 끝을 표시하는 토큰을 문장에 추가해주겠다.\n",
        "\n",
        "데이터셋 내에 존재하는 문장 길이에 대한 분포를 시각해보자."
      ],
      "metadata": {
        "id": "XZQbnDvvJ7YZ"
      },
      "id": "XZQbnDvvJ7YZ"
    },
    {
      "cell_type": "code",
      "source": [
        "text_len = [len(s.split()) for s in data['text']]\n",
        "summary_len = [len(s.split()) for s in data['headlines']]\n",
        "\n",
        "print('text의 최소 길이 : {}'.format(np.min(text_len)))\n",
        "print('text의 최대 길이 : {}'.format(np.max(text_len)))\n",
        "print('text의 평균 길이 : {}'.format(np.mean(text_len)))\n",
        "print('headlines의 최소 길이 : {}'.format(np.min(summary_len)))\n",
        "print('headlines의 최대 길이 : {}'.format(np.max(summary_len)))\n",
        "print('headlines의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
        "\n",
        "plt.figure(1, figsize=(10,4))\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(text_len, bins = 40)\n",
        "plt.xlabel('length of text')\n",
        "plt.ylabel('number of text')\n",
        "plt.title('text')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(summary_len, bins = 40)\n",
        "plt.xlabel('length of headlines')\n",
        "plt.ylabel('number of headlines')\n",
        "plt.title('headlines')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "YuD9AOZ9KXnI",
        "outputId": "27a70c02-964d-42d2-cf83-62e8e43b2933"
      },
      "id": "YuD9AOZ9KXnI",
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text의 최소 길이 : 1\n",
            "text의 최대 길이 : 60\n",
            "text의 평균 길이 : 35.09968483123221\n",
            "headlines의 최소 길이 : 1\n",
            "headlines의 최대 길이 : 16\n",
            "headlines의 평균 길이 : 9.299532330215534\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text와 headlines 데이터 모두 한쪽으로 치우침 없이 정규분포를 이루고 있다. \n",
        "headlines의 경우 최대, 최소길이의 차이도 크지않지만 text 길이가 45이상인 경우의 샘플과 headlines길이가 12이상인 샘플이 드물게 나타나고있다. 임의로 최대 길이 설정하여 어느정도 샘플이 삭제되는지 알아보자.\n",
        "\n"
      ],
      "metadata": {
        "id": "dRMxQzbBM43O"
      },
      "id": "dRMxQzbBM43O"
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_len = 50\n",
        "headlines_max_len = 12\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s.split()) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
        "\n",
        "below_threshold_len(text_max_len, data['text'])\n",
        "below_threshold_len(headlines_max_len,  data['headlines'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlasnbafKYfI",
        "outputId": "c5faeb33-0de1-4e47-d7c9-aec84068eafb"
      },
      "id": "DlasnbafKYfI",
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.9998576657177715\n",
            "전체 샘플 중 길이가 12 이하인 샘플의 비율: 0.9880337535583571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "99% 정도의 데이터가 존재하므로 이대로 최대길이를 설정해도 될것 같다."
      ],
      "metadata": {
        "id": "UJT_jIdUOS7i"
      },
      "id": "UJT_jIdUOS7i"
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnruB0ZEKYV4",
        "outputId": "878a2633-14ff-419b-aeb9-1923a89263fc"
      },
      "id": "TnruB0ZEKYV4",
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플수 : 97169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-6. 시작 토큰과 종료 토큰 추가하기\n",
        "\n",
        "seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가해주어야한다.\n",
        "- 토큰\n",
        " - 시작 토큰은 sostoken\n",
        " - 종료 토큰은 eostoken \n",
        "\n",
        "- headlins 데이터에 시작, 종료 토큰을 붙여 생성할 디코더 데이터\n",
        " - decoder_input 열 : 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장\n",
        " - decoder_target 열 : 디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장\n"
      ],
      "metadata": {
        "id": "WLLbhYF4RhwF"
      },
      "id": "WLLbhYF4RhwF"
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
        "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
        "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AyDzWJ3aKYLI",
        "outputId": "2950e548-3129-439c-bd88-4eea8211d8a1"
      },
      "id": "AyDzWJ3aKYLI",
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6e36f119-2847-4017-9bdf-ee87c2132e01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "      <th>decoder_input</th>\n",
              "      <th>decoder_target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upgrad learner switches to career in ml al wit...</td>\n",
              "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
              "      <td>sostoken upgrad learner switches to career in ...</td>\n",
              "      <td>upgrad learner switches to career in ml al wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delhi techie wins free food from swiggy for on...</td>\n",
              "      <td>kunal shah credit card bill payment platform c...</td>\n",
              "      <td>sostoken delhi techie wins free food from swig...</td>\n",
              "      <td>delhi techie wins free food from swiggy for on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new zealand end rohit sharma led india match w...</td>\n",
              "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
              "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
              "      <td>new zealand end rohit sharma led india match w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aegon life iterm insurance plan helps customer...</td>\n",
              "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
              "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
              "      <td>aegon life iterm insurance plan helps customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
              "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
              "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
              "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e36f119-2847-4017-9bdf-ee87c2132e01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e36f119-2847-4017-9bdf-ee87c2132e01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e36f119-2847-4017-9bdf-ee87c2132e01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           headlines  ...                                     decoder_target\n",
              "0  upgrad learner switches to career in ml al wit...  ...  upgrad learner switches to career in ml al wit...\n",
              "1  delhi techie wins free food from swiggy for on...  ...  delhi techie wins free food from swiggy for on...\n",
              "2  new zealand end rohit sharma led india match w...  ...  new zealand end rohit sharma led india match w...\n",
              "3  aegon life iterm insurance plan helps customer...  ...  aegon life iterm insurance plan helps customer...\n",
              "5  rahat fateh ali khan denies getting notice for...  ...  rahat fateh ali khan denies getting notice for...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장해준다."
      ],
      "metadata": {
        "id": "A-pKykuJS14S"
      },
      "id": "A-pKykuJS14S"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = np.array(data['text']) # 인코더의 입력\n",
        "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
        "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
      ],
      "metadata": {
        "id": "bYYEwSC-S45S"
      },
      "id": "bYYEwSC-S45S",
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-7. 훈련 데이터와 테스트 데이터를 분리\n",
        "train_test_split함수로 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리해주었다."
      ],
      "metadata": {
        "id": "tMxRKQSjZWxq"
      },
      "id": "tMxRKQSjZWxq"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "encoder_input_train, encoder_input_test = train_test_split(encoder_input, test_size=0.2, random_state=121 )\n",
        "decoder_input_train, decoder_input_test = train_test_split(decoder_input, test_size=0.2,  random_state=121 )\n",
        "decoder_target_train, decoder_target_test = train_test_split(decoder_target, test_size=0.2, random_state=121 )\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENEgKnRLWl6X",
        "outputId": "3f0d9d46-e1b0-4bfa-9fd2-c269eca18abf"
      },
      "id": "ENEgKnRLWl6X",
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 개수 : 77735\n",
            "훈련 레이블의 개수 : 77735\n",
            "테스트 데이터의 개수 : 19434\n",
            "테스트 레이블의 개수 : 19434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-8. 단어사전(vocabulary) 만들기 및 정수 인코딩\n",
        "\n",
        "학습을 위해 정제한 텍스트데이터를 숫자로 바꾸는 작업을 해주겠다. 이를 위해서는 각 단어에 고유한 정수를 맵핑하기 위해 단어 사전을 만들고 해당 사전과 관련하여 인코딩 작업을 해주면된다.\n",
        "\n",
        "우선, Keras의 토크나이저를 사용하여 원문에 해당되는 encoder_input_train에 대해서 단어 사전을 만들어주겠다."
      ],
      "metadata": {
        "id": "gPD3F0_qTC5z"
      },
      "id": "gPD3F0_qTC5z"
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
      ],
      "metadata": {
        "id": "tiLcGs56V6LH"
      },
      "id": "tiLcGs56V6LH",
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit_on_texts함수 호출시 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 생성되었다. 이 단어집합(src_tokenizer.word_index)에서 사용빈도가 낮은 단어들은 훈련 데이터에서 제외할 것이다.\n",
        "\n",
        "등장 빈도수가 9회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해보고 비중이 작으면 관련 단어를 삭제해도 무방할것 같다.\n",
        "- src_tokenizer.word_counts.items() : 단어와 각 단어의 등장 빈도수"
      ],
      "metadata": {
        "id": "CvPfO88GaV7M"
      },
      "id": "CvPfO88GaV7M"
    },
    {
      "cell_type": "code",
      "source": [
        "def word_counts(src_tokenizer, threshold):\n",
        "    total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
        "    rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "    rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "    # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "    for key, value in src_tokenizer.word_counts.items():\n",
        "        total_freq = total_freq + value\n",
        "\n",
        "        # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "        if(value < threshold):\n",
        "            rare_cnt = rare_cnt + 1\n",
        "            rare_freq = rare_freq + value\n",
        "\n",
        "    print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
        "    print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "    print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "    print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "    print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "word_counts(src_tokenizer, 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPo7-qlAV58X",
        "outputId": "f7326583-5688-4af6-df3a-bfa62529c574"
      },
      "id": "JPo7-qlAV58X",
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 69234\n",
            "등장 빈도가 8번 이하인 희귀 단어의 수: 49884\n",
            "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 19350\n",
            "단어 집합에서 희귀 단어의 비율: 72.05130427246729\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.259271744483268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder_input_train에는 69000여 개의 단어가 있다. 등장 빈도가 threshold 값인 9회 미만, 즉 8회 이하인 단어들은 단어 집합에서 72%정도 차지하고 있지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 4.25%밖에 되지 않는다.\n",
        "\n",
        "그래서 등장 빈도가 8회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거하겠다. 그리고 위에서 이를 제외한 단어 집합의 크기를 19000여개로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 19,000으로 제한해서 단어사전을 다시 만들겠다. "
      ],
      "metadata": {
        "id": "xxw-O8-3bDg-"
      },
      "id": "xxw-O8-3bDg-"
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = 19000\n",
        "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 19,000으로 제한\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성"
      ],
      "metadata": {
        "id": "_H59DQTYcacO"
      },
      "id": "_H59DQTYcacO",
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_train[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8b_JOfgomny",
        "outputId": "4765bbfd-ebab-439a-fe86-f0ba1730b984"
      },
      "id": "b8b_JOfgomny",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['japanese tabloid magazine spa apologised published list ranking women universities easy convince students sex drinking parties woman launched online campaign seeking apology suspension sales offensive issue would like apologise using sensational language appeal readers magazine said'\n",
            " 'class female student hacked death assailant outside school madhya pradesh anuppur district police said friday assailant used sword severe neck girl walking school appear examination police detained man earlier accused eve teasing victim'\n",
            " 'former reserve bank india chief raghuram rajan thursday said never resigned post step term ended talking demonetisation rajan said still entire data talk demonetisation date fixed demonetisation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "texts_to_sequences 적용 : 텍스트를 시퀀스로 인코딩\n"
      ],
      "metadata": {
        "id": "uDtKheyzci4Q"
      },
      "id": "uDtKheyzci4Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
        "\n",
        "# 잘 진행되었는지 샘플 출력\n",
        "print(encoder_input_train[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiLspJNAcaTJ",
        "outputId": "b5dcdf37-1da4-496f-ba0c-3f662ed85511"
      },
      "id": "CiLspJNAcaTJ",
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 13, 65, 9254, 5056, 23, 160, 1815, 12, 822, 1910, 3018, 3893, 6936, 1678, 163, 14733, 27, 176, 65, 3777, 832, 95, 85, 1096, 4636, 784, 98, 2173, 3608, 954, 6936], [8494, 36, 56, 45, 13296, 3707, 1881, 58, 72, 380, 1264, 1250, 805, 60, 3019, 909, 7756, 15953, 4438, 1891, 1523, 1196, 8, 376, 1881, 4, 17383, 759, 684, 10, 12508, 801, 1239], [11, 1092, 323, 4752, 3894, 28, 51, 3437, 1316, 2353, 12186, 13297, 2588, 3685, 3437, 813, 1392, 5135, 6662, 650, 9255, 6662, 89, 515, 2205, 1543, 714, 2159, 687, 11471, 5136, 6662, 612, 983, 1649, 813, 13297, 399, 2515, 983, 1649, 813, 10426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "headlines 데이터에 대해서도 동일한 작업을 진행해주겠다. 단 headlines 데이터의 경우 text보다 문장이 짧기 때문에 희귀단어의 임계치를 7로 조정했다."
      ],
      "metadata": {
        "id": "q9x139cphiu1"
      },
      "id": "q9x139cphiu1"
    },
    {
      "cell_type": "code",
      "source": [
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "\n",
        "word_counts(tar_tokenizer, 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_U5jGM_iHNG",
        "outputId": "a86786d2-0f56-4ff5-ff9b-8920edf8aa51"
      },
      "id": "U_U5jGM_iHNG",
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 29989\n",
            "등장 빈도가 6번 이하인 희귀 단어의 수: 20507\n",
            "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9482\n",
            "단어 집합에서 희귀 단어의 비율: 68.38173997132282\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.3745010772089685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder_input_train에는 3만여 개의 단어가 있다. 등장 빈도가 6회 이하인 단어들은 단어 집합에서 68% 이상을 차지하고 있고 등장 빈도로 차지하는 비중은 5.3%로 확인된다. 이 단어를 제외한 단어집합의 크기는 9482개으로 9000건정도로 설정하면 될 것 같다."
      ],
      "metadata": {
        "id": "cunOSXMqiu3u"
      },
      "id": "cunOSXMqiu3u"
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = 9000\n",
        "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
        "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
        "\n",
        "# 잘 변환되었는지 확인\n",
        "print('input ',decoder_input_train[:3])\n",
        "print('decoder ',decoder_target_train[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iu6PQ5XiHJG",
        "outputId": "b8ee605d-488e-4d66-8a59-a15ab8da25c2"
      },
      "id": "5iu6PQ5XiHJG",
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input  [[1, 39, 4653, 174, 10, 100, 125, 1249, 4, 28], [1, 696, 3542, 1250, 19, 336, 4822, 108], [1, 2511, 3747, 50, 1391, 165, 1699, 4823, 2146]]\n",
            "decoder  [[39, 4653, 174, 10, 100, 125, 1249, 4, 28, 2], [696, 3542, 1250, 19, 336, 4822, 108, 2], [2511, 3747, 50, 1391, 165, 1699, 4823, 2146, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-9. 빈문장이 있는지 다시한번 더 확인\n",
        "인코더, 디코더 데이터는 이제 빈도수가 높은 단어의 인덱스로 구성되어있다. 그런데 빈도수가 낮은 단어가 있는 샘플은 빈 샘플로 남아 있을 것이므로 해당 샘플도 제거해주어야한다. \n",
        "\n",
        "디코더 데이터의 경우 토큰을 하나씩 추가했으므로 문자길이가 1인경우 빈샘플로 볼수 있다."
      ],
      "metadata": {
        "id": "JTbXtUXWkGEU"
      },
      "id": "JTbXtUXWkGEU"
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = [index for index, sentence in enumerate(encoder_input_train) if len(sentence) == 0]\n",
        "drop_test = [index for index, sentence in enumerate(encoder_input_train) if len(sentence) == 0]\n",
        "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
        "print('삭제할 테스트 데이터의 개수 :', len(drop_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDwHfLTBlvWG",
        "outputId": "403b73fd-5f52-4e97-bf7f-9b464fe88d26"
      },
      "id": "MDwHfLTBlvWG",
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "삭제할 훈련 데이터의 개수 : 0\n",
            "삭제할 테스트 데이터의 개수 : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더 데이터의경우 빈샘플이 없다. 디코더 데이터를 살펴보자"
      ],
      "metadata": {
        "id": "hs7Q3zQOlyvv"
      },
      "id": "hs7Q3zQOlyvv"
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
        "\n",
        "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
        "print('삭제할 테스트 데이터의 개수 :', len(drop_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDGBMAbsl6-m",
        "outputId": "09ae9ad7-7f43-48f3-b451-40b7214654d6"
      },
      "id": "MDGBMAbsl6-m",
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "삭제할 훈련 데이터의 개수 : 1\n",
            "삭제할 테스트 데이터의 개수 : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
        "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
        "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
        "\n",
        "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
        "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
        "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwOsKBuq5ZCS",
        "outputId": "7b4345e8-7d88-4c51-d2fe-66ad685c6046"
      },
      "id": "rwOsKBuq5ZCS",
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 개수 : 77734\n",
            "훈련 레이블의 개수 : 77734\n",
            "테스트 데이터의 개수 : 19434\n",
            "테스트 레이블의 개수 : 19434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더 데이터의 경우 빈샘플이 하나발견되어 삭제해주었다. 빈샘플이 별로 없는 것은대부분의 문장이 빈도수가 많은 단어들로 구성되었기때문인것 같다."
      ],
      "metadata": {
        "id": "nYnAsfAimTW1"
      },
      "id": "nYnAsfAimTW1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1-10. 문장의 길이 맞추기 (pad_sequences)\n",
        "\n",
        "이제 정수로 인코딩된 샘플의 서로 다른 길이를 같은 길이로 맞춰주는 패딩 작업을 해주겠다. 미리 정해둔 최대 길이를 설정하고 패딩은 훈련시 효과가 좋은 앞공간에 넣어주도록 할것이다."
      ],
      "metadata": {
        "id": "xxUGubKymgTQ"
      },
      "id": "xxUGubKymgTQ"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen=headlines_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen=headlines_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen=headlines_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen=headlines_max_len, padding='post')"
      ],
      "metadata": {
        "id": "TuK1qTSXS5fT"
      },
      "id": "TuK1qTSXS5fT",
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. 모델 설계하기"
      ],
      "metadata": {
        "id": "eu8vtz2qIGyX"
      },
      "id": "eu8vtz2qIGyX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2-1. 기본 seq2seq 모델 설계하기\n",
        "\n",
        "<img src=\"https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG\" width=\"600\" height=\"200\"/>\n",
        "\n",
        "\n",
        "seq2seq는 크게 인코더와 디코더라는 두 개의 모듈로 구성된다.\n",
        "- 인코더 \n",
        " - 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 컨텍스트 벡터(context vector)로 만든다.\n",
        " - 인코더의  hidden state와 cell state를 디코더에 넘겨준다.\n",
        "\n",
        "- 디코더 \n",
        " - Language Model(언어모델) : 문장(시퀀스)에 확률을 부여하는 모델이다.\n",
        " - 컨텍스트 벡터는 디코더셀의 첫번째 은닉 상태에 사용된다.\n",
        " - 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력한다.\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "Gb8ud2rGTV28"
      },
      "id": "Gb8ud2rGTV28"
    },
    {
      "cell_type": "markdown",
      "id": "804a4199",
      "metadata": {
        "id": "804a4199"
      },
      "source": [
        "### 1-2-2. 어텐션 메커니즘 사용하여 모델 개선하기\n",
        "\n",
        "기본적인 seq2seq모델의 디코더 출력층을 바꿔서 성능을 높일수 있다. 이를 위해 어텐션 메커니즘을 활용해보겠다.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Chandan-Reddy-2/publication/329464533/figure/fig3/AS:701043021197314@1544153089772/An-attention-based-seq2seq-model.ppm\" width=\"600\" height=\"400\"/>\n",
        "\n",
        "\n",
        "- **어텐션 메커니즘 (Attention Mechanism)**\n",
        " - 하나의 고정된 크기의 벡터에 모든 정보를 압축하여 정보 손실이 발생하는 seq2seq 모델의 단점을 보완\n",
        " - 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다.\n",
        " - **attention : 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 본다.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "TPU사용을 위해 한 함수내에 인코더, 디코더모델을 함께 구현해보겠다."
      ],
      "metadata": {
        "id": "lNHcd8Y_ITw2"
      },
      "id": "lNHcd8Y_ITw2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼 파라미터\n",
        "- embedding_dim = 128 : 임베딩벡터의 차원\n",
        "- hidden_size = 256 : LSTM 뉴런의 갯수\n",
        "\n",
        "인코더 구조\n",
        "- Embedding 레이어\n",
        " - 18000개 단어로 이루어진 text 데이터에 임베딩(문장 최대길이:40)\n",
        "- LSTM 레이어 \n",
        " - return_sequences : 앞쪽 순환층의 모든 은닉상태를 출력하기위해 설정\n",
        " - return_state : 마지막 시점의 은닉 상태 출력\n",
        " - dropout : 은닉층에 있는 셀의 입력에 dropout 적용\n",
        " - recurrent_dropout : 순환되는 은닉상태에 대해 dropout 적용, 즉, dropout을 레이어가 아닌 time step마다 해주는 방식. 기술적문제로 GPU로 훈련시키지 못하는 점이 있다.\n",
        "\n",
        "디코더 구조 - 기본적인 seq2seq 모델\n",
        "- Embedding 레이어\n",
        " - 9000개 단어로 이루어진 headlines 데이터에 임베딩(문장 최대길이:12)\n",
        "- LSTM 레이어 \n",
        " - initial_state : 컨텍스트 벡터인 인코더의 hidden state와 cell state의 값 설정\n",
        "- Dense 레이어(디코더 출력층)\n",
        " -  headlines 9000개 단어중 text데이터를 요약하는데 필요한 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어하므로 Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용한다.\n",
        "\n",
        "**디코더 구조중 출력층 바꾸기** - 어텐션 메커니즘의 활용\n",
        " - [Bahdanau 스타일의 어텐션](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention)\n",
        " - 표준 seq2seq 모델은 인코더 마지막 hidden state만 디코더의 컨텍스트 벡터로 사용되기 때문에 일반적으로 긴 입력 시퀀스를 정확하게 처리하기 어렵다.\n",
        " - 어텐션 메커니즘은 디코딩 처리시 입력시퀀스의 모든 hidden state를 활용하여 특정 요소를 선택하여 출력을 생성한다.\n",
        " - attn_layer : 어텐션 함수의 입력으로 인코더의 hidden state들과 디코더의 hidden state를 사용\n",
        " - 어텐션 함수가 리턴한 값과 디코더의 hidden state를 함께 활용하여 요약문을 구성할 단어를 예측한다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        " - return_sequences : 앞쪽 순환층의 모든 은닉상태를 출력하기위해 설정\n",
        " - return_state : 마지막 시점의 은닉 상태 출력\n",
        " - dropout : 은닉층에 있는 셀의 입력에 dropout 적용\n",
        " - recurrent_dropout : 순환되는 은닉상태에 대해 dropout 적용, 기술적문제로 GPU로 훈련시키지 못하는 점이 있다. "
      ],
      "metadata": {
        "id": "0tSPQaQQcBoK"
      },
      "id": "0tSPQaQQcBoK"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import AdditiveAttention\n",
        "\n",
        "def createModel():\n",
        "    embedding_dim = 128\n",
        "    hidden_size = 256\n",
        "\n",
        "    #------------------------------------------- 인코더\n",
        "    encoder_inputs = Input(shape=(text_max_len,))\n",
        "\n",
        "    # 인코더의 임베딩 층\n",
        "    enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "    # 인코더의 LSTM 1\n",
        "    encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
        "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "    # 인코더의 LSTM 2\n",
        "    encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "    # 인코더의 LSTM 3\n",
        "    encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "\n",
        "    #------------------------------------------- 디코더 \n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    # 디코더의 임베딩 층\n",
        "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "    # 디코더의 LSTM\n",
        "    decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    # 디코더의 출력층\n",
        "    '''\n",
        "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "    decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
        "\n",
        "    # 모델 정의\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "    model.summary()\n",
        "    '''\n",
        "    #-------------------------------------------  디코더의 출력층 개선 - 어텐션 적용\n",
        "    # 어텐션 층(어텐션 함수)\n",
        "    attn_layer = AdditiveAttention(name='attention_layer')\n",
        "\n",
        "    # 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
        "    attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "    # 어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "    # 디코더의 출력층\n",
        "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "    decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
        "\n",
        "    # 모델 정의\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Jddx2CpEBOWU"
      },
      "id": "Jddx2CpEBOWU",
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-3. 모델 훈련"
      ],
      "metadata": {
        "id": "dzdslYMOkdWH"
      },
      "id": "dzdslYMOkdWH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EarlyStopping**\n",
        "- 조기 종료, 훈련진행시 특정 조건이 충족되면 훈련을 멈추는 역할\n",
        " > es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "- monitor(관찰대상) : 위 코드에서는 val_loss(검증 데이터의 손실)\n",
        "- patience : 종료 조건, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측"
      ],
      "metadata": {
        "id": "wDwfjLiLkGVn"
      },
      "id": "wDwfjLiLkGVn"
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = createModel()\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')  \n",
        "    es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "    history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "            validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
        "            batch_size=256, callbacks=[es], epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzBXVf59BfDM",
        "outputId": "9e2ef0cf-2131-49be-9934-96344d3612e7"
      },
      "id": "ZzBXVf59BfDM",
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_37 (InputLayer)          [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_30 (Embedding)       (None, 50, 128)      2432000     ['input_37[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_60 (LSTM)                 [(None, 50, 256),    394240      ['embedding_30[0][0]']           \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " input_38 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_61 (LSTM)                 [(None, 50, 256),    525312      ['lstm_60[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " embedding_31 (Embedding)       (None, None, 128)    1152000     ['input_38[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_62 (LSTM)                 [(None, 50, 256),    525312      ['lstm_61[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_63 (LSTM)                 [(None, None, 256),  394240      ['embedding_31[0][0]',           \n",
            "                                 (None, 256),                     'lstm_62[0][1]',                \n",
            "                                 (None, 256)]                     'lstm_62[0][2]']                \n",
            "                                                                                                  \n",
            " attention_layer (AdditiveAtten  (None, None, 256)   256         ['lstm_63[0][0]',                \n",
            " tion)                                                            'lstm_62[0][0]']                \n",
            "                                                                                                  \n",
            " concat_layer (Concatenate)     (None, None, 512)    0           ['lstm_63[0][0]',                \n",
            "                                                                  'attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, None, 9000)   4617000     ['concat_layer[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,040,360\n",
            "Trainable params: 10,040,360\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "304/304 [==============================] - 62s 125ms/step - loss: 5.5388 - val_loss: 5.1461\n",
            "Epoch 2/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 4.9591 - val_loss: 4.7332\n",
            "Epoch 3/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 4.6034 - val_loss: 4.4476\n",
            "Epoch 4/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 4.3449 - val_loss: 4.2729\n",
            "Epoch 5/50\n",
            "304/304 [==============================] - 21s 68ms/step - loss: 4.1436 - val_loss: 4.1294\n",
            "Epoch 6/50\n",
            "304/304 [==============================] - 21s 68ms/step - loss: 3.9753 - val_loss: 4.0318\n",
            "Epoch 7/50\n",
            "304/304 [==============================] - 22s 72ms/step - loss: 3.8336 - val_loss: 3.9335\n",
            "Epoch 8/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.7090 - val_loss: 3.8574\n",
            "Epoch 9/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.6042 - val_loss: 3.8015\n",
            "Epoch 10/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.5077 - val_loss: 3.7614\n",
            "Epoch 11/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.4205 - val_loss: 3.7189\n",
            "Epoch 12/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.3474 - val_loss: 3.6898\n",
            "Epoch 13/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.2784 - val_loss: 3.6491\n",
            "Epoch 14/50\n",
            "304/304 [==============================] - 21s 68ms/step - loss: 3.2111 - val_loss: 3.6307\n",
            "Epoch 15/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.1546 - val_loss: 3.6052\n",
            "Epoch 16/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 3.0987 - val_loss: 3.5945\n",
            "Epoch 17/50\n",
            "304/304 [==============================] - 21s 67ms/step - loss: 3.0433 - val_loss: 3.5708\n",
            "Epoch 18/50\n",
            "304/304 [==============================] - 21s 68ms/step - loss: 2.9963 - val_loss: 3.5609\n",
            "Epoch 19/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.9532 - val_loss: 3.5537\n",
            "Epoch 20/50\n",
            "304/304 [==============================] - 21s 68ms/step - loss: 2.9139 - val_loss: 3.5355\n",
            "Epoch 21/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.8761 - val_loss: 3.5323\n",
            "Epoch 22/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.8404 - val_loss: 3.5268\n",
            "Epoch 23/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.8054 - val_loss: 3.5263\n",
            "Epoch 24/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.7711 - val_loss: 3.5216\n",
            "Epoch 25/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.7377 - val_loss: 3.5140\n",
            "Epoch 26/50\n",
            "304/304 [==============================] - 21s 69ms/step - loss: 2.7042 - val_loss: 3.5128\n",
            "Epoch 27/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.6744 - val_loss: 3.5125\n",
            "Epoch 28/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.6460 - val_loss: 3.5098\n",
            "Epoch 29/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.6187 - val_loss: 3.5063\n",
            "Epoch 30/50\n",
            "304/304 [==============================] - 21s 67ms/step - loss: 2.5919 - val_loss: 3.5058\n",
            "Epoch 31/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.5660 - val_loss: 3.5080\n",
            "Epoch 32/50\n",
            "304/304 [==============================] - 20s 67ms/step - loss: 2.5458 - val_loss: 3.5103\n",
            "Epoch 00032: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "val_loss가 3.5063에서 더이상 줄어들지않아 epoch 32번째에서 멈추었다. 조기종료로 그 이후로 과적합을 막아주었다.\n",
        "훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는지 시각화를 통해 확인해보자. "
      ],
      "metadata": {
        "id": "-JxyynkKkl5w"
      },
      "id": "-JxyynkKkl5w"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xM3J85IaIaCZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "37ae2c68-7a94-4347-f29b-0fe2e0ac0939"
      },
      "id": "xM3J85IaIaCZ",
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-4. 인퍼런스 모델 설계하기"
      ],
      "metadata": {
        "id": "tzEgLXmYIntO"
      },
      "id": "tzEgLXmYIntO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 훈련이 끝났으므로 테스트를 진행해보겠다. 테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 아래와 같이 미리 준비한다."
      ],
      "metadata": {
        "id": "McR-4wxflDsW"
      },
      "id": "McR-4wxflDsW"
    },
    {
      "cell_type": "code",
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
      ],
      "metadata": {
        "id": "90CTwu33SRQw"
      },
      "id": "90CTwu33SRQw",
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**인퍼런스?**\n",
        "- 인퍼런스(Inference, 추론)란 학습을 마친 모델로 실제 새로운 입력 데이터에 적용하여 결과를 내놓는 과정을 말한다.\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdspFH4%2FbtrfpsIYHs2%2FekOoKY0DhGloYhriaGrkfK%2Fimg.png\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "따라서 seq2seq 모델은 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 한다.\n",
        "\n",
        "훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한 번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비했다. 하지만 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이 만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야한다. 이때는 인코더 모델과 디코더 모델을 분리해서 설계한다."
      ],
      "metadata": {
        "id": "s6D26j51lQKk"
      },
      "id": "s6D26j51lQKk"
    },
    {
      "cell_type": "code",
      "source": [
        "def createInferenceModel():\n",
        "    embedding_dim = 128\n",
        "    hidden_size = 256\n",
        "\n",
        "    #------------------------------------------- 인코더\n",
        "    encoder_inputs = Input(shape=(text_max_len,))\n",
        "    # 인코더의 임베딩 층\n",
        "    enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "\n",
        "    # 인코더의 LSTM 1\n",
        "    encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
        "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "    # 인코더의 LSTM 2\n",
        "    encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "    # 인코더의 LSTM 3\n",
        "    encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
        "    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "    # added for Inference\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "    # 이전 시점의 상태들을 저장하는 텐서\n",
        "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "\n",
        "    #------------------------------------------- 디코더 \n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    # 디코더의 임베딩 층\n",
        "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "    dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "    # 디코더의 LSTM\n",
        "\n",
        "    # 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "    # 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.    \n",
        "    decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "    #-------------------------------------------  디코더의 출력층 - 어텐션 적용\n",
        "    # 어텐션 층(어텐션 함수)\n",
        "    decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
        "    attn_layer = AdditiveAttention(name='attention_layer')\n",
        "    attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
        "    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "    # 디코더의 출력층\n",
        "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "    decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
        "\n",
        "    # 최종 디코더 모델\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "        [decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "encoder_model, decoder_model = createInferenceModel()"
      ],
      "metadata": {
        "id": "u1GRlv1TlN2o"
      },
      "id": "u1GRlv1TlN2o",
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수"
      ],
      "metadata": {
        "id": "I-exrXx9rA43"
      },
      "id": "I-exrXx9rA43"
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>에 해당하는 토큰 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_token!='eostoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headlines_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 상태를 업데이트 합니다.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "tZUMLOJNrGc5"
      },
      "id": "tZUMLOJNrGc5",
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-5. 모델 테스트\n",
        "정수 시퀀스를 텍스트 시퀀스로 변환하여 결과를 확인할수있도록 함수를 만들어주겠다. \n",
        "- Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외\n",
        "- headlines의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외"
      ],
      "metadata": {
        "id": "F1YHU4Dlrq5d"
      },
      "id": "F1YHU4Dlrq5d"
    },
    {
      "cell_type": "code",
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2text(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if (i!=0):\n",
        "            temp = temp + src_index_to_word[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2summary(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
        "            temp = temp + tar_index_to_word[i] + ' '\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "WrBTa8nvr4IH"
      },
      "id": "WrBTa8nvr4IH",
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0f15f6b8",
      "metadata": {
        "id": "0f15f6b8"
      },
      "source": [
        "## 1-6. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
        "원래의 요약문(headlines 열) 추상적 요약의 결과를 비교해보자."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(110, 115):\n",
        "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
        "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
        "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP2Hux51lX8e",
        "outputId": "d2c19550-75ea-45ba-a304-c33873b653be"
      },
      "id": "CP2Hux51lX8e",
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문 : rapper badshah appearance koffee karan revealed two women time made revelation game segment take shots guilty something badshah revealed dated nine women far \n",
            "실제 요약 : have been with two women at the same time reveals \n",
            "예측 요약 :  strapped northeast introduced bush produces development judgement subway iits slept slept\n",
            "\n",
            "\n",
            "원문 : talking former employee sexual harassment allegations director vikas bahl filmmaker bahl partner phantom films anurag kashyap said believe completely bahl done anurag added whatever happened wrong handle well failed cannot blame anyone said support \n",
            "실제 요약 : what vikas bahl has done is anurag on harassment row \n",
            "예측 요약 :  sharing strapped northeast introduced bush produces nations farooq farooq newborns newborns\n",
            "\n",
            "\n",
            "원문 : official trailer shabana azmi starrer hollywood film black prince unveiled ongoing cannes film festival film based maharaja duleep singh last king sikh empire also starring film release english hindi punjabi july \n",
            "실제 요약 : trailer of shabana azmi hollywood film unveiled at cannes \n",
            "예측 요약 :  strapped northeast gangraped parts scheme formed harassment jump azaan holds sparked\n",
            "\n",
            "\n",
            "원문 : pm narendra modi friday said government believes act east act fast india east policy adding moving isolation integration way rising india said proud government worked emotional integration also demographic dividend northeast \n",
            "실제 요약 : we believe in act east and act fast for india east pm \n",
            "예측 요약 :  northeast sharing cast vodafone cast slapping vodafone workout workout scindia revoke\n",
            "\n",
            "\n",
            "원문 : icc monday said five international captains reported approaches spot fixing bookies past months four full member countries one non full member country icc anti corruption general manager alex marshall said investigations last months eight involve players suspects added \n",
            "실제 요약 : approached int captains in past year for fixing icc \n",
            "예측 요약 :  sharing hostage reddit baba hawking hawking kota clothes offering sacha salad\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과적으로 요약이 잘된 것 같진 않다. 이전 LMS에서 진행했던 아마존 리뷰 요약보다 val_loss값이 1.84 였는데 그보다 손실값이 2배나 높게 나왔다.\n",
        "\n",
        "> sharing strapped northeast introduced bush produces nations farooq farooq newborns newborns\n",
        "\n",
        "요약 내용중에 farooq, newborns와 같은 단어가 반복되기도 하고 원문의 키워드도 요약내용중에 거의 보이지 않는다. 더 많은 개선이 필요한것 같다."
      ],
      "metadata": {
        "id": "03GXPMt2zMjV"
      },
      "id": "03GXPMt2zMjV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. 추출적 요약(Extractive Summarization)\n",
        "\n",
        "- [summa packages](https://summanlp.github.io/textrank/)"
      ],
      "metadata": {
        "id": "C0DnGOjLdKO7"
      },
      "id": "C0DnGOjLdKO7"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install summa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGezp0KZxCrP",
        "outputId": "b792c8b0-8897-418c-c047-07bd360cfd45"
      },
      "id": "FGezp0KZxCrP",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54412 sha256=a07ded1d34a0e1dddb9fa30ff07a5a2b86525fb95c35a6a1bc8658075ac15f23\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from summa.summarizer import summarize"
      ],
      "metadata": {
        "id": "HwiZIWkGxFzP"
      },
      "id": "HwiZIWkGxFzP",
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "09384212",
      "metadata": {
        "id": "09384212"
      },
      "source": [
        "## 2-1. Summa을 이용해서 추출적 요약해보기\n",
        "\n",
        "Summa 패키지\n",
        "- 추출적 요약을 위한 모듈인 summarize를 제공\n",
        "- 본문에 존재하는 단어구, 문장을 뽑아서 요약\n",
        "\n",
        "summarize 함수\n",
        "- 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행한다. 따라서 문장 구분이 되어있지 않은 원문을 바로 입력으로 넣을 수 있다. \n",
        "\n",
        "파라미터 \n",
        "- text (str) : 요약할 테스트.\n",
        "- ratio (float, optional) – 요약문에서 원본에서 선택되는 문장 비율. 0~1 사이값\n",
        "- words (int or None, optional) – 출력에 포함할 단어 수.\n",
        " - ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시한다.\n",
        "- split (bool, optional) – True면 문장 list / False는 조인(join)된 문자열을 반환"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
      ],
      "metadata": {
        "id": "uKsdIkVsf5hb"
      },
      "id": "uKsdIkVsf5hb",
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "summarize 함수는 내부적으로 토큰화를 수행하므로 원문을 넣어주기위해 다시 data를 로드하였다. "
      ],
      "metadata": {
        "id": "tqEUQ2l_hJPq"
      },
      "id": "tqEUQ2l_hJPq"
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(data2['text'][1],  ratio=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "32yO-3rAfJN0",
        "outputId": "19ef6b41-2fd8-410d-b7bd-a8b005f403ad"
      },
      "id": "32yO-3rAfJN0",
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.'"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추출적요약을위해 ratio를 설정해주었는데 원문이 짧아서 ratio를 0.3이하로 설정시 요약결과가 나오지않아 원본에서 선택되는 문장비율을 0.4정도로 잡아주었다. "
      ],
      "metadata": {
        "id": "JtaI0GZehWkz"
      },
      "id": "JtaI0GZehWkz"
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "id": "2787e3a8",
      "metadata": {
        "id": "2787e3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e535e29-60d3-4faa-c210-6c42369c8d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문 : German multinational engineering and electronics firm Bosch has made its first investment in India in Bengaluru-based deep-tech startup SimYog. SimYog has raised about Ã¢ÂÂ¹6.3 crore in the funding round, with participation from early-stage venture capital firm Ideaspring Capital. Incubated at the Indian Institute of Science (IISc), it provides design and sign-off tools for automotive electronics.\n",
            "실제 요약 : Bosch makes its 1st India investment in IISc spinoff SimYog\n",
            "예측 요약 : \n",
            "\n",
            "\n",
            "원문 : Mumbai-headquartered talent technology startup Shortlist has raised $2 million in a Series A round of funding. The round was led by Blue Haven Initiative, with participation from Compass Venture Capital, Zephyr Acorn among others. Founded by Simon Desjardins, Paul Breloff and Matt Schnuck, Shortlist screens candidates using predictive chat-based interviews and online competency-based assessments.\n",
            "실제 요약 : Mumbai's talent tech startup Shortlist raises $2 million\n",
            "예측 요약 : Mumbai-headquartered talent technology startup Shortlist has raised $2 million in a Series A round of funding.\n",
            "\n",
            "\n",
            "원문 : MIT engineers have developed a jelly-like ingestible pill that expands inside the stomach and can be used to monitor conditions like cancers and ulcers for up to a month. The pill, made using two types of hydrogels, is embedded with a sensor that continuously tracks the stomach's temperature. It can be removed by drinking a calcium-ion solution that shrinks it.\n",
            "실제 요약 : MIT makes pill that expands in stomach to track cancer, ulcer\n",
            "예측 요약 : MIT engineers have developed a jelly-like ingestible pill that expands inside the stomach and can be used to monitor conditions like cancers and ulcers for up to a month.\n",
            "\n",
            "\n",
            "원문 : New York's Columbia University neuro-engineers claim to have developed a system that can convert human brain signals directly into speech using artificial intelligence. Researchers revealed a computer algorithm 'vocoder' was used. \"This is the same technology used by Amazon Echo and Apple Siri to give verbal responses to our questions,\" senior author of the study Nima Mesgarani said.\n",
            "실제 요약 : Brain signals converted directly into speech using AI: Study\n",
            "예측 요약 : New York's Columbia University neuro-engineers claim to have developed a system that can convert human brain signals directly into speech using artificial intelligence.\n",
            "\n",
            "\n",
            "원문 : A team of IIT-Roorkee scientists is testing a floating device that can generate electricity from flowing surface water of rivers. Instead of using water falling from height to turn turbines like in traditional dams, the prototype uses velocity of flowing river, professor RP Saini said. \"Flowing water can generate hundred times more power than wind of same velocity,\" Saini added.\n",
            "실제 요약 : IIT Roorkee tests device to generate electricity from river flow\n",
            "예측 요약 : \"Flowing water can generate hundred times more power than wind of same velocity,\" Saini added.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(100, 105):\n",
        "    print(\"원문 :\", data2['text'][i])\n",
        "    print(\"실제 요약 :\", data2['headlines'][i])\n",
        "    print(\"예측 요약 :\", summarize(data2['text'][i],  ratio=0.4))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추출적 요약의 경우 실제 요약과 예측요약이 엇비슷한 경우가 많은것 같다. \n",
        "단 헤드라인으로 쓰기에 제목이 좀 길어서 좀더 축약적인 표현을 쓰지 못하는 부분이 아쉽다. \n",
        "\n",
        " - 실제 요약 : Mumbai's talent tech startup Shortlist raises \\$2 million\n",
        " - 예측 요약 : Mumbai-headquartered talent technology startup Shortlist has raised $2 million in a Series A round of funding.\n",
        "\n"
      ],
      "metadata": {
        "id": "dOfMfvMj2b3_"
      },
      "id": "dOfMfvMj2b3_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 추상적 요약과 추출적 요약 비교\n",
        "추출적요약을 잘활용하면 좀더 유연한 문장을 만들수 있을것 같지만 아직 내가 만든 모델의 한계가 있어 추출적 요약 비교할때 문법완성도면이나 키워드 포함면에서 많은 부분이 부족했다.\n",
        "\n",
        " - 핵심단어 포함 결과 비교 : 추출적 요약은 기존문장에서 문장을 꺼내 왔기때문에 핵심단어를 많이 포함하고 있다. 하지만 기존문장을 간단히 요약할수 있는 다른 단어를 만드는 데에는 한계가 있어보인다.\n",
        " \n",
        " - 문법완성도 결과 비교 : 추출적 요약이 역시 원문을 가져왔으므로 문법도 그대로 적용되어 읽을때 어색함이 없다. 추상적 요약은 같은 단어가 문장에 반복되는 경우가 많고,  문법에 어긋난 표현도 자주 나타난다. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sqVb9txmeCQU"
      },
      "id": "sqVb9txmeCQU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 정리.\n",
        "\n",
        "- 이번 프로젝트에서는 처음으로 자연어 처리를 위한 모델 두개를 연결하여 진행해보았다. 입력으로 들어오는 원문과 출력으로 나오는 요약문의 결과 길이가 다르고 출력단에서 반영해야하는 부분이 원문과 원문을 요약한 데이터이기 때문에 인코더, 디코더 모델을 따로 설계하여 쓰는 것을 알게되었다.\n",
        " >model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "\n",
        "- 테스트를 위해 인퍼런스 모델을 따로 만들어 테스트를 진행했는데, 아직은 기존 모델과 차이점을 이해하기 어려웠지만 현업에서 많이 쓰이는 부분이라 앞으로 더 익숙해져야할 것 같다.\n",
        "\n",
        "- TPU의 활용 : LSTM에서 recurrent_dropout를 True로 설정할 경우 GPU를 사용하지못하게 되었다. 대신 일구님의 도움으로 코랩에서 지원하는 TPU를 사용해 보았는데 3배 이상 빠르게 모델을 학습시킬수 있어 많은 도움이 되었다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MilRAu960MyI"
      },
      "id": "MilRAu960MyI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Reference\n",
        "- [Sequence-to-Sequence](https://wikidocs.net/24996)\n",
        "- https://ichi.pro/ko/cheoeumbuteo-chusangjeog-in-tegseuteu-yoyag-ihae-205262993770163\n"
      ],
      "metadata": {
        "id": "sCyFxA-XrDhl"
      },
      "id": "sCyFxA-XrDhl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Exp08_NewsSummarization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}