{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e90ae73",
   "metadata": {},
   "source": [
    "# 멋진 작사가 만들기\n",
    "---\n",
    "\n",
    "이번 프로젝트에서는 49개의 노래가사를 학습하여 스스로 가사를 만드는 인공지는 작사가를 만들어볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615074b9",
   "metadata": {},
   "source": [
    "# 1. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6fc78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be201b1",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. 데이터셋 구성\n",
    "RNN모델에 들어갈 데이터셋을 구성하는 순서를 요약해보면 다음과 같다\n",
    "1. 데이터 읽어오기 :학습할 가사내용을 파일에서 읽어오기\n",
    "2. 문장 필터링 : 불필요한 특수기호나 필요없는 구간은 제외\n",
    "3. 정규표현식을 이용한 corpus(말뭉치) 생성\n",
    "4. 토큰화 : tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "5. Dataset 만들기 : tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798f76f",
   "metadata": {},
   "source": [
    "### 2.1. 데이터 읽어오기\n",
    "glob 를 활용하여 lyrics 폴더 하위의 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "756c1838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 개수: 49\n",
      "데이터 크기: 187088\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+ '/aiffel/project/exp04_LyricistAI/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "n_file_count = 0\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        n_file_count += 1\n",
    "    \n",
    "print('파일 개수:', n_file_count)\n",
    "print('데이터 크기:', len(raw_corpus))              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea254c7e",
   "metadata": {},
   "source": [
    "총 49개의 파일에서 18만(187,088)개 이상의 문장을 읽어 raw_corpus 리스트에 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9be4aa",
   "metadata": {},
   "source": [
    "### 2.2 문장 필터링\n",
    "raw_corpus 리스트의 문장에서 필터링할 내용을 미리 걸러주자. 필터링 대상 문장은 모델이 학습하기에 부적절한 내용을 담은 중복된 문장이나 빈문장에 해당한다. \n",
    "단, 18만개 이상의 문장을 모두다 들여다 볼수 없으므로 랜덤하게 살펴보도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e451fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Looks like freedom but it feels like death',\n",
       " \"It's something in between, I guess\",\n",
       " \"It's closing time\",\n",
       " '(Closing time)',\n",
       " '(Closing time)',\n",
       " '(Closing time) Yeah I missed you since the place got wrecked',\n",
       " 'By the winds of change and the weeds of sex',\n",
       " 'Looks like freedom but it feels like death',\n",
       " \"It's something in between, I guess\",\n",
       " \"It's closing time Yeah we're drinking and we're dancing\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[1100:1110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d230e3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When He healed the blind and crippled, did they see?',\n",
       " '',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " 'The multitude wanted to make Him king, put a crown upon His head',\n",
       " 'Why did He slip away to a quiet place instead?',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " '',\n",
       " 'When He rose from the dead, did they believe?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[20000:20010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df378a0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A snake is summer's treason,\",\n",
       " '   And guile is where it goes.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'XX.',\n",
       " '',\n",
       " 'Could I but ride indefinite,']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[115100:115110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee757c4b",
   "metadata": {},
   "source": [
    "문장을 들여다보니 노래가사의 특성상 계속 반복되는 문장이 많은것을 볼수 있다. 그리고 노래사이에 (Closing time) 표현이 들어가있거나 빈문장이 발견되었다. 따라서 아래내용을 전처리 해주어야겠다.\n",
    "- 빈문장 제외\n",
    "- 소괄호( ) 문장 제외\n",
    "- 중복 문장 제외\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16099e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링후 문장 개수 :  116025\n",
      "['You love it how I beat it', \"My imagination's running wild It feels like ooh\", \"44, I'm a pawn fella\", 'Blowing through the air', 'Gimme, Gimme, More Give me more, give me more', 'We got no class, no taste, no shirt, and shit faced', 'I opened up my eyes', \"I'm twisted: door knob\", \"And I'll fulfill all your fantasies\", 'Uh you know I need that wet mouth']\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence.strip()) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[0] == \"(\": continue  # 문장의 시작이 ( 인 문장은 건너뜁니다.    \n",
    "    \n",
    "    new_corpus.append(sentence)\n",
    "\n",
    "new_corpus = list(set(new_corpus)) #중복문장은 set 자료로 형변환하여 제외시킨다\n",
    "print('필터링후 문장 개수 : ', len(new_corpus))\n",
    "print(new_corpus[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a25e0c",
   "metadata": {},
   "source": [
    "필터링하여 깔끔한 문장이 출력되었다. 18만건이상이던 문장도 11만6000건으로 줄었다. 이제 이문장들을 대상으로 **정규표현식**을 써서 좀 더 다듬어주어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c07ba3",
   "metadata": {},
   "source": [
    "### 2.3 정규표현식을 이용한 corpus(말뭉치) 생성\n",
    " 1. 소문자로 바꾸고, 양쪽 공백을 지운다\n",
    " 2. 특수문자 양쪽에 공백을 넣는다\n",
    " 3. 여러개의 공백은 하나의 공백으로 바꾼다\n",
    " 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다\n",
    " 5. 다시 양쪽 공백을 지운다\n",
    " 6. 문장 시작에는 \\<start>, 끝에는 \\<end>를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "213ed5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fae1f1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> you love it how i beat it <end>',\n",
       " '<start> my imagination s running wild it feels like ooh <end>',\n",
       " '<start> , i m a pawn fella <end>',\n",
       " '<start> blowing through the air <end>',\n",
       " '<start> gimme , gimme , more give me more , give me more <end>',\n",
       " '<start> we got no class , no taste , no shirt , and shit faced <end>',\n",
       " '<start> i opened up my eyes <end>',\n",
       " '<start> i m twisted door knob <end>',\n",
       " '<start> and i ll fulfill all your fantasies <end>',\n",
       " '<start> uh you know i need that wet mouth <end>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_corpus = []\n",
    "\n",
    "for sentence in new_corpus:    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    last_corpus.append(preprocessed_sentence)\n",
    "\n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "last_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf924c",
   "metadata": {},
   "source": [
    "정규화를 통해 말뭉치(last_corpus)가 완성되었다\n",
    "- 말뭉치란(코퍼스, corpus) ?\n",
    "> 분석 대상인 비정형 텍스트 데이터이다. 여러 단어들로 이루어진 문장의 뭉치를 일컫는다.\n",
    "\\<start>와 \\<end> 심볼을 넣은 점이 특이한데, 문장의 시작과 끝을 표기해 주어 모델에서 문장의 단위를 알수 있게 한것 같다.\n",
    "\n",
    "- 단어사전(어휘사전)이란?\n",
    "> 문장에서 고유한 단어를 뽑아 만든 목록. 토큰화 진행시 단어사전에 없는 단어는 unk로 설정한다.\n",
    "\n",
    "위의 정제된 문장을 이용해서 단어사전을 만들어보자. 사전을 만든 후 토큰화까지 진행해보겠다. 프로젝트 요구사항에 맞게 단어장의 크기는 12000으로 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422e8cd",
   "metadata": {},
   "source": [
    "### 2.4 토큰화 \n",
    "### 토큰화란?\n",
    "> 이미지를 학습시킬때 픽셀정보가 수치화되어 있듯이 우리가 준비한 데이터셋도 수치화해주는 작업이 필요하다. 여기서는 텐서플로우의 tf.keras.preprocessing.text.Tokenizer 라이브러리를 사용하여 데이터를 토큰화하여 단어사전을 만들고 데이터를 숫자로 변경해 줄것이다. 숫자로 변환된 데이터를 텐서(tensor) 라고 칭한다고 한다.\n",
    "\n",
    "### 토큰화 처리 순서\n",
    "1. 토크나이즈 객체 생성 : Tokenizer 함수  \n",
    "  - num_words : 단어사전의 단어의 개수 (빈도수가 높은 12000개의 단어를 저장)\n",
    "  - filters : 토큰화전 필터링 처리하여 따로 설정하지 않음\n",
    "  - oov_token : 12000 단어에 포함되지 못한 단어는 '\\<unk>'로 설정\n",
    " > oov란 ? out of vocabulary 즉, train 데이터에서는 등장하지 않았던 단어를 말한다\n",
    "\n",
    "2. fit_on_texts \n",
    " - 116025개 문장의 last_corpus으로 단어사전 생성\n",
    " \n",
    "3. texts_to_sequences\n",
    " - 텍스트를 시퀀스로 변환하기, 즉, 단어별로 인덱스를 부여하는 것이다.\n",
    " - tokenizer의 word_index 속성을 확인해보면 딕셔너리 데이터로 해당 텍스트와 숫자가 매칭된 것을 확인할 수 있다.\n",
    "\n",
    "![이미지](https://codetorial.net/tensorflow/_images/natural_language_processing_in_tensorflow_02.png)\n",
    "\n",
    "4. pad_sequences\n",
    " - 서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들어줌\n",
    " - padding : 시퀸스 최대길이가 15인데 그보다 짧은 문장의 경우 padding 파라미터로 post를 설정하여 '0' 을 채우도록 하였다\n",
    " - maxlen : 시퀀스의 최대길이를 15로 설정\n",
    " > **지나치게 긴문장이 있을 경우 다른 문장들이 무의미한 Padding을 가지게 되므로 이번 프로젝트에서는 문장에 포함될 단어의 갯수를 15개로 제한했다.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "243db27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15) \n",
    "   \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(last_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743791c6",
   "metadata": {},
   "source": [
    "말뭉치의 텍스트가 수치로 변했는지 tokenizer.index_word를 통해 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3291408b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27552"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, ':', tokenizer.index_word[idx])\n",
    "    if idx>=10:break\n",
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9803e7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116025,\n",
       " 15,\n",
       " array([  2,   7,  37,  11,  80,   5, 344,  11,   3,   0,   0,   0,   0,\n",
       "          0,   0], dtype=int32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor), len(tensor[0]), tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dddb1",
   "metadata": {},
   "source": [
    "116025개 문장이 텐서로 생성되었다. 각 텐서의 길이는 15이고 문장의 길이가 짦은경우 뒤에 0으로 패딩이 붙은것도 확인이된다.\n",
    "tensor[0]를 보면 첫번째 문장 \\<start> you love it how i beat it \\<end>의 각 단어가 숫자로 어떻게 매칭되어있는지 확인 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d80b1",
   "metadata": {},
   "source": [
    "### 2.5 Dataset 만들기 \n",
    "준비된 텐서를 소스와 타겟으로 분리할 차례이다. tf.data.Dataset.from_tensor_slices()를 이용해 텐서를 tf.data.Dataset객체로 변환하고 모델에 학습시키면될 것 이다.\n",
    "\n",
    "\n",
    "소스 문장(Source Sentence) : 자연어 처리에서 모델의 입력이 되는 문장\n",
    " - \\<start> this is sample sentence . 문장에 대한 텐서 ->  X_train\n",
    "    \n",
    "타겟 문장(Target Sentence) : 정답 역할을 하게 될 모델의 출력 문장\n",
    " - this is sample sentence . \\<end> 문장에 대한 텐서 -> y_train\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df9f1de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   7  37  11  80   5 344  11   3   0   0   0   0   0]\n",
      "[  7  37  11  80   5 344  11   3   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end> 또는 <pad>가 될것이다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36cdbe",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 평가 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35bc2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83a7a63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: (92820, 14)\n",
      "test dataset: (23205, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset:\", enc_train.shape)\n",
    "print(\"test dataset:\", enc_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880a94d",
   "metadata": {},
   "source": [
    "9만개의 데이터로 학습을 진행한후 테스트셋 데이터로 작사가 잘되는지 테스트를 해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc87ce",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 모델 학습\n",
    "\n",
    "이프로젝트에서 RNN 모델은 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성했다.\n",
    "\n",
    "### RNN 모델에 대한 간략한 이해\n",
    "입력 데이터\n",
    "- 단어 사전의 인덱스\n",
    "\n",
    "Embedding 레이어\n",
    "- 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다\n",
    "- 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용된다\n",
    "- vocab_size : 단어사전의 크기\n",
    "- embedding_size : 256, 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    " -  크기가 2라면 차갑다: [0.0, 1.0], 뜨겁다: [1.0, 0.0], 미지근하다: [0.5, 0.5] 이렇게 단어의 추상적 특징을 수치로 벡터공간에서 표현한다.\n",
    "\n",
    "LSTM 레이어\n",
    "- RNN에서 기억 값에 대한 가중치를 제어하며\n",
    "- hidden_size : 1024, hidden state 의 차원수\n",
    "- 데이터를 이해하는 신경망 노드의 수\n",
    "- return_sequences=True : 자신에게 입력된 시퀀스 길이만큼 동일한 시퀀스를 출력(긴문장을 생성해야하므로)\n",
    "- return_sequences=False : 1개의 벡터만 출력\n",
    "\n",
    "Dense 레이어\n",
    " - 출력값 : 출력층에서 표현되는 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(1024, return_sequences=True, dropout=0.2)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(512, return_sequences=True, dropout=0.2)\n",
    "        self.rnn_3 = tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.2)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    #call함수에서 입력에 대한 출력값이 다음 레이어의 입력으로 순환되어있다.\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.rnn_3(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa495603",
   "metadata": {},
   "source": [
    "- 손실 함수와 옵티마이저 설정\n",
    " - 옵티마이저로 다양한 신경망에서 효과적으로 쓰이는 Adam을 사용했고,\n",
    " - 손실함수로는 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행할때 쓰이는 SparseCategoricalCrossentropy를 썼다.\n",
    "- 미니배치 \n",
    " - BUFFER_SIZE(116025)사이즈만큼 모두 학습을 진행하면 시간이 오래걸리기때문에 미니배치로 나누어 진행하였다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0263e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "embedding_size = 512 \n",
    "\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history = model.fit(enc_train, dec_train, batch_size=BATCH_SIZE, epochs=10, validation_data=(enc_val, dec_val), verbose=2 )\n",
    "#history = model.fit(enc_train, dec_train,  epochs=10, validation_data=(enc_val, dec_val), verbose=2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a2bed",
   "metadata": {},
   "source": [
    "10 Epoch 안에 val_loss 값을 2.2 수준이하인 이 되었다. hidden_size과 embedding_size을 줄이니 훈련속도는 빨라졌지만 훈련 후의 손실값이 3.0이 넘었다. 학습을 위해 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4aea2",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. 모델 테스트\n",
    "학습한 모델이 어느정도의 작곡실력을 보여주는지 테스트해 볼 차례이다. 관련함수를 만들고 테스트를 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c0ee7",
   "metadata": {},
   "source": [
    "모델을 테스트하기위해 첫 단어를 \\<start> 토큰과 함께 작곡 문장 생성 함수(generate_text)에 넘겨주어야한다.\n",
    "\n",
    "generate_text 함수의 실행 순서\n",
    "- 입력받은 단어를 텐서로 변환한다.\n",
    "- 생성값이 \\<end>토큰인지 비교하기위해 미리 \\<end>토큰의 텐서를 설정해둔다.\n",
    "- 예측된 값 중 가장 높은 확률인 word index를 입력텐서뒤에 붙여 test_tensor를 만든다\n",
    "- 이렇게 예측 단어를 이전 텐서에 붙이는 과정을 반복한다.\n",
    "- 문장 생성을 마치는 경우\n",
    " - \\<end>을 예측한경우\n",
    " - 문장에 포함된 텐서 길이가 max_len(15)가 되면 종료\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f671b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "255f2667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> that s the way it was meant to be <end> '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> that\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc8d1c",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 정리\n",
    "\n",
    "- 비정형데이터중 이미지에 대한 머신러닝 학습만 하다가 텍스트에 대한 학습을 진행하니 생소한 점이 많았다. 이미지의 경우 픽셀에 대한 데이터가 RGB값으로 수치화하는 과정이 쉽게 이해가 되었는데 텍스트의 경우 텐서(벡터)로 변환하는 과정이 까다로웠던것 같다. 단어의 의미를 다차원의 공간에 표시하여 나타낼수있다는 부분도 재미있었다. \n",
    "\n",
    "- 전처리시 사용했던 정규화 기법이나 그외 어간추출, 형태소분석등 텍스트 데이터에 대한 깊은 이해가 선행되어야할것 같다.\n",
    "\n",
    "- RNN모델에서 학습할 입력데이터로 이전 레이어의 데이터가 순환적으로 연결되는 구조를 알게되었다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492fc13f",
   "metadata": {},
   "source": [
    "---\n",
    "# Reference\n",
    "- 토큰화 : [링크](https://codetorial.net/tensorflow/natural_language_processing_in_tensorflow_01.html)\n",
    "\n",
    "- NLP : [링크](https://velog.io/@tmddn0311/RNN-tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
