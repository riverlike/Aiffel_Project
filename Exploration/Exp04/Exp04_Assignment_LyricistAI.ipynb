{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff200b02",
   "metadata": {},
   "source": [
    "# 멋진 작사가 만들기\n",
    "---\n",
    "\n",
    "이번 프로젝트에서는 49개의 노래가사를 학습하여 스스로 가사를 만드는 인공지는 작사가를 만들어볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4a00f",
   "metadata": {},
   "source": [
    "# 1. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8ca59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007c786",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. 데이터셋 구성\n",
    "RNN모델에 들어갈 데이터셋을 구성하는 순서를 요약해보면 다음과 같다\n",
    "1. 데이터 읽어오기 :학습할 가사내용을 파일에서 읽어오기\n",
    "2. 문장 필터링 : 불필요한 특수기호나 필요없는 구간은 제외\n",
    "3. 정규표현식을 이용한 corpus(말뭉치) 생성\n",
    "4. 토큰화 : tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "5. Dataset 만들기 : tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15fb32",
   "metadata": {},
   "source": [
    "### 2.1. 데이터 읽어오기\n",
    "glob 를 활용하여 lyrics 폴더 하위의 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505e067a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 개수: 49\n",
      "데이터 크기: 187088\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+ '/aiffel/project/exp04_LyricistAI/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "n_file_count = 0\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        n_file_count += 1\n",
    "    \n",
    "print('파일 개수:', n_file_count)\n",
    "print('데이터 크기:', len(raw_corpus))              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c15fa",
   "metadata": {},
   "source": [
    "총 49개의 파일에서 18만(187,088)개 이상의 문장을 읽어 raw_corpus 리스트에 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996464a",
   "metadata": {},
   "source": [
    "### 2.2 문장 필터링\n",
    "raw_corpus 리스트의 문장에서 필터링할 내용을 미리 걸러주자. 필터링 대상 문장은 모델이 학습하기에 부적절한 내용을 담은 중복된 문장이나 빈문장에 해당한다. \n",
    "단, 18만개 이상의 문장을 모두다 들여다 볼수 없으므로 랜덤하게 살펴보도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a12b58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Looks like freedom but it feels like death',\n",
       " \"It's something in between, I guess\",\n",
       " \"It's closing time\",\n",
       " '(Closing time)',\n",
       " '(Closing time)',\n",
       " '(Closing time) Yeah I missed you since the place got wrecked',\n",
       " 'By the winds of change and the weeds of sex',\n",
       " 'Looks like freedom but it feels like death',\n",
       " \"It's something in between, I guess\",\n",
       " \"It's closing time Yeah we're drinking and we're dancing\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[1100:1110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68046ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When He healed the blind and crippled, did they see?',\n",
       " '',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " 'The multitude wanted to make Him king, put a crown upon His head',\n",
       " 'Why did He slip away to a quiet place instead?',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " 'Did they speak out against Him, did they dare?',\n",
       " '',\n",
       " 'When He rose from the dead, did they believe?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[20000:20010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee54e0aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A snake is summer's treason,\",\n",
       " '   And guile is where it goes.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'XX.',\n",
       " '',\n",
       " 'Could I but ride indefinite,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[115100:115110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8670a",
   "metadata": {},
   "source": [
    "문장을 들여다보니 노래가사의 특성상 계속 반복되는 문장이 많은것을 볼수 있다. 그리고 노래사이에 (Closing time) 표현이 들어가있거나 빈문장이 발견되었다. 따라서 아래내용을 전처리 해주어야겠다.\n",
    "- 빈문장 제외\n",
    "- 소괄호( ) 문장 제외\n",
    "- 중복 문장 제외\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20ebb0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링후 문장 개수 :  116025\n",
      "['I caress you, let you taste us, just so blissful listen', \"I'm on my way to see that girl of mine\", 'And when my Mac unloads', \"When we kiss And there's a weepy old willow\", \"He's thirsty when he drinks\", '    For want of a nail, the shoe was lost,', 'Hit me to tell me you get off at 10', \"I wonder if you'll let us stay with you She was just a little girl, not more than six or seven\", 'I’m the one watching you', \"I'ma hold you forever,\"]\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence.strip()) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[0] == \"(\": continue  # 문장의 시작이 ( 인 문장은 건너뜁니다.    \n",
    "    \n",
    "    new_corpus.append(sentence)\n",
    "\n",
    "new_corpus = list(set(new_corpus)) #중복문장은 set 자료로 형변환하여 제외시킨다\n",
    "print('필터링후 문장 개수 : ', len(new_corpus))\n",
    "print(new_corpus[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de48a9b",
   "metadata": {},
   "source": [
    "필터링하여 깔끔한 문장이 출력되었다. 18만건이상이던 문장도 11만6000건으로 줄었다. 이제 이문장들을 대상으로 **정규표현식**을 써서 좀 더 다듬어주어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1439c5",
   "metadata": {},
   "source": [
    "### 2.3 정규표현식을 이용한 corpus(말뭉치) 생성\n",
    " 1. 소문자로 바꾸고, 양쪽 공백을 지운다 : sentence.lower().strip()  \n",
    " 2. a-z가 아닌 모든 문자를 하나의 공백으로 바꾼다 : re.sub\n",
    " 3. 여러개의 공백은 하나의 공백으로 바꾼다 : re.sub\n",
    " 4. 다시 양쪽 공백을 지운다 : strip\n",
    " 5. 문장 시작에는 \\<start>, 끝에는 \\<end>를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3862ef93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()     \n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence) #\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59fafc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i caress you let you taste us just so blissful listen <end>',\n",
       " '<start> i m on my way to see that girl of mine <end>',\n",
       " '<start> and when my mac unloads <end>',\n",
       " '<start> when we kiss and there s a weepy old willow <end>',\n",
       " '<start> he s thirsty when he drinks <end>',\n",
       " '<start> for want of a nail the shoe was lost <end>',\n",
       " '<start> hit me to tell me you get off at <end>',\n",
       " '<start> i wonder if you ll let us stay with you she was just a little girl not more than six or seven <end>',\n",
       " '<start> i m the one watching you <end>',\n",
       " '<start> i ma hold you forever <end>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_corpus = []\n",
    "\n",
    "for sentence in new_corpus:    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    last_corpus.append(preprocessed_sentence)\n",
    "\n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "last_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3bf59",
   "metadata": {},
   "source": [
    "정규화를 통해 말뭉치(last_corpus)가 완성되었다\n",
    "- 말뭉치란(코퍼스, corpus) ?\n",
    "> 분석 대상인 비정형 텍스트 데이터이다. 여러 단어들로 이루어진 문장의 뭉치를 일컫는다.\n",
    "\\<start>와 \\<end> 심볼을 넣은 점이 특이한데, 문장의 시작과 끝을 표기해 주어 모델에서 문장의 단위를 알수 있게 한것 같다.\n",
    "\n",
    "- 단어사전(어휘사전)이란?\n",
    "> 문장에서 고유한 단어를 뽑아 만든 목록. 토큰화 진행시 단어사전에 없는 단어는 unk로 설정한다.\n",
    "\n",
    "위의 정제된 문장을 이용해서 단어사전을 만들어보자. 사전을 만든 후 토큰화까지 진행해보겠다. 프로젝트 요구사항에 맞게 단어장의 크기는 12000으로 설정했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dc8e5",
   "metadata": {},
   "source": [
    "### 2.4 토큰화 \n",
    "### 토큰화란?\n",
    "> 이미지를 학습시킬때 픽셀정보가 수치화되어 있듯이 우리가 준비한 데이터셋도 수치화해주는 작업이 필요하다. 여기서는 텐서플로우의 tf.keras.preprocessing.text.Tokenizer 라이브러리를 사용하여 데이터를 토큰화하여 단어사전을 만들고 데이터를 숫자로 변경해 줄것이다. 숫자로 변환된 데이터를 텐서(tensor) 라고 칭한다고 한다.\n",
    "\n",
    "### 토큰화 처리 순서\n",
    "1. 토크나이즈 객체 생성 : Tokenizer 함수  \n",
    "  - num_words : 단어사전의 단어의 개수 (빈도수가 높은 12000개의 단어를 저장)\n",
    "  - filters : 토큰화전 필터링 처리하여 따로 설정하지 않음\n",
    "  - oov_token : 12000 단어에 포함되지 못한 단어는 '\\<unk>'로 설정\n",
    " > oov란 ? out of vocabulary 즉, train 데이터에서는 등장하지 않았던 단어를 말한다\n",
    "\n",
    "2. fit_on_texts \n",
    " - 116025개 문장의 last_corpus으로 단어사전 생성\n",
    " \n",
    "3. texts_to_sequences\n",
    " - 텍스트를 시퀀스로 변환하기, 즉, 단어별로 인덱스를 부여하는 것이다.\n",
    " - tokenizer의 word_index 속성을 확인해보면 딕셔너리 데이터로 해당 텍스트와 숫자가 매칭된 것을 확인할 수 있다.\n",
    "\n",
    "![이미지](https://codetorial.net/tensorflow/_images/natural_language_processing_in_tensorflow_02.png)\n",
    "\n",
    "4. pad_sequences\n",
    " - 서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들어줌\n",
    " - padding : 시퀸스 최대길이가 15인데 그보다 짧은 문장의 경우 padding 파라미터로 post를 설정하여 '0' 을 채우도록 하였다\n",
    " - maxlen : 시퀀스의 최대길이를 15로 설정\n",
    " > **지나치게 긴문장이 있을 경우 다른 문장들이 무의미한 Padding을 가지게 되므로 이번 프로젝트에서는 문장에 포함될 단어의 갯수를 15개로 제한했다.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b0d8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15) \n",
    "   \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(last_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290208ad",
   "metadata": {},
   "source": [
    "말뭉치의 텍스트가 수치로 변했는지 tokenizer.index_word를 통해 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07046869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : the\n",
      "6 : you\n",
      "7 : and\n",
      "8 : a\n",
      "9 : to\n",
      "10 : it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27547"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, ':', tokenizer.index_word[idx])\n",
    "    if idx>=10:break\n",
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0016713d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116025,\n",
       " 15,\n",
       " array([  2,   4,  19,  18,  12,  84,   9,  61,  16,  80,  17, 212,   3,\n",
       "          0,   0], dtype=int32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor), len(tensor[1]), tensor[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd4191",
   "metadata": {},
   "source": [
    "116025개 문장이 텐서로 생성되었다. 각 텐서의 길이는 15이고 문장의 길이가 짦은경우 뒤쪽에 0으로 패딩이 붙은것도 확인이된다.\n",
    "tensor[1]를 보면 첫번째 문장 \\<start> i m on my way to see that girl of mine \\<end>의 각 단어가 숫자로 어떻게 매칭되어있는지 확인 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade17aab",
   "metadata": {},
   "source": [
    "### 2.5 Dataset 만들기 \n",
    "준비된 텐서를 소스와 타겟으로 분리할 차례이다. tf.data.Dataset.from_tensor_slices()를 이용해 텐서를 tf.data.Dataset객체로 변환하고 모델에 학습시키면될 것 이다.\n",
    "\n",
    "\n",
    "소스 문장(Source Sentence) : 자연어 처리에서 모델의 입력이 되는 문장\n",
    " - \\<start> this is sample sentence . 문장에 대한 텐서 ->  X_train\n",
    "    \n",
    "타겟 문장(Target Sentence) : 정답 역할을 하게 될 모델의 출력 문장\n",
    " - this is sample sentence . \\<end> 문장에 대한 텐서 -> y_train\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71d7daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    4 4778    6   60    6  675  133   32   27 9213  426    3    0]\n",
      "[   4 4778    6   60    6  675  133   32   27 9213  426    3    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end> 또는 <pad>가 될것이다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751b4e2",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 평가 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13cef7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b432c01e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: (92820, 14)\n",
      "test dataset: (23205, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset:\", enc_train.shape)\n",
    "print(\"test dataset:\", enc_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c10e0",
   "metadata": {},
   "source": [
    "9만개의 학습데이터와 2만 3000건의 검증데이터로 학습을 진행해보겠다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa27e91",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 모델 학습\n",
    "\n",
    "이프로젝트에서 RNN 모델은 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성했다.\n",
    "\n",
    "### RNN 모델에 대한 간략한 이해\n",
    "입력 데이터\n",
    "- 단어 사전의 인덱스\n",
    "\n",
    "Embedding 레이어\n",
    "- 1차원의 벡터를 2차원으로 바꿔준다\n",
    "- 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용된다\n",
    "- vocab_size : 단어사전의 크기\n",
    "- embedding_size : 256, 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    " -  크기가 2라면 차갑다: [0.0, 1.0], 뜨겁다: [1.0, 0.0], 미지근하다: [0.5, 0.5] 이렇게 단어의 추상적 특징을 수치로 벡터공간에서 표현한다.\n",
    "\n",
    "LSTM 레이어\n",
    "- RNN에서 기억 값에 대한 가중치를 제어한다\n",
    "- hidden_size : 1024, hidden state 의 차원수\n",
    "- 데이터를 이해하는 신경망 노드의 수\n",
    "- return_sequences=True : 자신에게 입력된 시퀀스 길이만큼 동일한 시퀀스를 출력(긴문장을 생성해야하므로)\n",
    "- return_sequences=False : 1개의 벡터만 출력\n",
    "\n",
    "Dense 레이어\n",
    " - 출력값 : 출력층에서 표현되는 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ea06ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0328b071",
   "metadata": {},
   "source": [
    "- 손실 함수와 옵티마이저 설정\n",
    " - 옵티마이저로 다양한 신경망에서 효과적으로 쓰이는 Adam을 사용했고,\n",
    " - 손실함수로는 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행할때 쓰이는 SparseCategoricalCrossentropy를 썼다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68d22783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2901/2901 - 111s - loss: 3.4423 - val_loss: 3.2073\n",
      "Epoch 2/10\n",
      "2901/2901 - 107s - loss: 3.0604 - val_loss: 3.0720\n",
      "Epoch 3/10\n",
      "2901/2901 - 107s - loss: 2.8625 - val_loss: 3.0209\n",
      "Epoch 4/10\n",
      "2901/2901 - 107s - loss: 2.6750 - val_loss: 3.0219\n",
      "Epoch 5/10\n",
      "2901/2901 - 107s - loss: 2.4864 - val_loss: 3.0575\n",
      "Epoch 6/10\n",
      "2901/2901 - 107s - loss: 2.3020 - val_loss: 3.1195\n",
      "Epoch 7/10\n",
      "2901/2901 - 107s - loss: 2.1266 - val_loss: 3.1847\n",
      "Epoch 8/10\n",
      "2901/2901 - 107s - loss: 1.9601 - val_loss: 3.2580\n",
      "Epoch 9/10\n",
      "2901/2901 - 107s - loss: 1.8034 - val_loss: 3.3547\n",
      "Epoch 10/10\n",
      "2901/2901 - 107s - loss: 1.6574 - val_loss: 3.4552\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history = model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val, dec_val), verbose=2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dcabd",
   "metadata": {},
   "source": [
    "학습이 진행되면서 손실이 줄어들었다가 다시늘어나는 과대적합이 나타났다. embedding_size, hidden_size 사이즈를 줄이거나, 늘리는것으로 손실값이 줄어들지 않아 많은 시행착오를 했지만 원했던 결과가 나오지않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c8dc8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2901/2901 - 314s - loss: 3.3492 - val_loss: 3.1134\n",
      "Epoch 2/10\n",
      "2901/2901 - 312s - loss: 2.9457 - val_loss: 2.9887\n",
      "Epoch 3/10\n",
      "2901/2901 - 322s - loss: 2.6664 - val_loss: 2.9592\n",
      "Epoch 4/10\n",
      "2901/2901 - 315s - loss: 2.3449 - val_loss: 2.9924\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size, hidden_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history = model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val, dec_val), verbose=2, callbacks=[early_stopping] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1354cc",
   "metadata": {},
   "source": [
    "과적합이 계속적으로 나타나 조기종료를 추가하여 일찍 학습은 마쳤다. 원하는 성능까지는 이르지 못했지만 일단 이모델로 테스트를 진행해보았다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a2428",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. 모델 테스트\n",
    "학습한 모델이 어느정도의 작곡실력을 보여주는지 테스트해 볼 차례이다. 관련함수를 만들고 테스트를 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa86ed8",
   "metadata": {},
   "source": [
    "모델을 테스트하기위해 첫 단어를 \\<start> 토큰과 함께 작곡 문장 생성 함수(generate_text)에 넘겨주어야한다.\n",
    "\n",
    "generate_text 함수의 실행 순서\n",
    "- 입력받은 단어를 텐서로 변환한다.\n",
    "- 생성값이 \\<end>토큰인지 비교하기위해 미리 \\<end>토큰의 텐서를 설정해둔다.\n",
    "- 예측된 값 중 가장 높은 확률인 word index를 입력텐서뒤에 붙여 test_tensor를 만든다\n",
    "- 이렇게 예측 단어를 이전 텐서에 붙이는 과정을 반복한다.\n",
    "- 문장 생성을 마치는 경우\n",
    " - \\<end>을 예측한경우\n",
    " - 문장에 포함된 텐서 길이가 max_len(15)가 되면 종료\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df9bba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):    \n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3699c637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so <end> '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0edd461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> that s why i m in love with judas <end> '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb1e6736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m a <unk> <end> '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> You know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3fe7366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> earth is gone <end> '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> Earth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b158f3",
   "metadata": {},
   "source": [
    "제법 노래로 지어부르기 좋은 가사들을 만드는것 같다. 지구를 넣으니 지구가 사라졌다니 참 창의적(?)인것 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658b868",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 정리\n",
    "\n",
    "- 비정형데이터중 이미지에 대한 머신러닝 학습만 하다가 텍스트에 대한 학습을 진행하니 생소한 점이 많았다. 이미지의 경우 픽셀에 대한 데이터를 RGB값으로 수치화하는 과정이 쉽게 이해되는 반면에, 텍스트의 경우 텐서(벡터)로 변환하는 과정이 까다롭고 어려웠던 점이 많았다. 단어의 의미를 다차원의 공간에 표시하여 나타낼수있다는 부분도 어려웠지만 비슷한 의미가 가까운 거리에, 반대되는 의미는 먼거리에 위치시킨다는 이론이 와닿았다.\n",
    "\n",
    "\n",
    "- 전처리시 사용했던 정규화 기법을 적용하기위해 텍스트 데이터에 대한 깊은 이해가 선행되어야할것 같다.\n",
    "\n",
    "\n",
    "- RNN모델에서 학습할 입력데이터로 이전 레이어의 데이터가 순환적으로 연결되는 구조를 알게되었다. \n",
    "\n",
    "\n",
    "- validation loss를 2.2로 만들기까지 레이어도 추가해보고, 히든유닛수나 임베딩사이즈를 조절해보았는데 크게 향상되지않았다. 히든유닛수나 임베딩사이즈를 변경하는것으로 성능이 나아지지않았다.  더 성능을 높이고 싶지만 하이퍼파라미터의 문제보다 근본적인 문제가 있는듯하여  여기까지하고 마치도록 하겠다. 빅데이터까지는 아니지만 많은양의 데이터로 학습후 결과값이 나오기까지 아주 오랜시간이 걸리는 것을 몸소 체험해본 기나긴 프로젝트였다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5641083",
   "metadata": {},
   "source": [
    "---\n",
    "# Reference\n",
    "- 토큰화 : [링크](https://codetorial.net/tensorflow/natural_language_processing_in_tensorflow_01.html)\n",
    "\n",
    "- NLP : [링크](https://velog.io/@tmddn0311/RNN-tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
