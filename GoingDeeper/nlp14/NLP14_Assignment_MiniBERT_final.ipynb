{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dCkncUhUV2q"
      },
      "source": [
        "# mini BERT 만들기\n",
        "\n",
        "- BERT pretrained model\n",
        " - vocab size : 8000\n",
        " - 전체 파라미터 사이즈 : 1M \n",
        " - 학습횟수 : 10 Epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7Qd3JXSgIo0",
        "outputId": "2b444b38-967a-4230-ec95-e1ec6fc08098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "1.21.6\n",
            "1.3.5\n",
            "3.2.2\n",
            "2.0.9\n",
            "2.2.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "import numpy\n",
        "import pandas\n",
        "import matplotlib\n",
        "import json\n",
        "import re\n",
        "\n",
        "print(tensorflow.__version__)\n",
        "print(numpy.__version__)\n",
        "print(pandas.__version__)\n",
        "print(matplotlib.__version__)\n",
        "print(json.__version__)\n",
        "print(re.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q-BeW1Rz-swF"
      },
      "outputs": [],
      "source": [
        "!mkdir 'models'\n",
        "!mkdir 'data'\n",
        "!unzip -qq '/content/drive/MyDrive/Colab Notebooks/Aiffel/03_GoingDeeper/14/data_234M/kowiki.txt.zip' -d './data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCmjicgq-u5V",
        "outputId": "e2db3f50-f740-4db8-c964-75f39503d54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 12.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTzsXEEvUVW7"
      },
      "source": [
        "# 1. Tokenizer 준비\n",
        "- BERT의 MLM 학습용 데이터 \n",
        " - 한글 나무 위키 코퍼스로부터 8000의 vocab_size를 갖는 sentencepiece 모델\n",
        " - BERT에 사용되는 주요 특수문자가 vocab에 포함됨 : [SEP],[CLS],[MASK]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aBbOvhX--sYI"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "corpus_file = '/content/data/kowiki.txt'\n",
        "prefix = 'ko_8000'\n",
        "vocab_size = 8000\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" +  # 7 : 특수토큰 + 사용자정의토큰\n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpbtR4pg_DDt",
        "outputId": "7d842ef2-480f-40f4-87d5-55ec0732fb84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data_dir = '/content/data'\n",
        "model_dir = '/content/models'\n",
        "\n",
        "# vocab loading\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(f\"ko_8000.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr479rOO_PK1"
      },
      "source": [
        "토크나이징 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7r0JFik_SCk",
        "outputId": "167958fe-4114-4eb1-e0c9-8456a8548bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 8000\n"
          ]
        }
      ],
      "source": [
        "# 특수 token 7개를 제외한 나머지 tokens 들\n",
        "vocab_list = []\n",
        "for id in range(7, len(vocab)):\n",
        "    if not vocab.is_unknown(id):\n",
        "        vocab_list.append(vocab.id_to_piece(id))\n",
        "\n",
        "print('vocab size:', len(vocab_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncOvXC-7_Sxs",
        "outputId": "23d7c6c0-0da9-436c-a4dd-348f7abd38a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "vocab_list[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gBJUQM0gSMd"
      },
      "source": [
        "# 2. 데이터 전처리 (1) MASK 생성\n",
        "BERT의 MLM(Masked LM)에 필요한 빈칸(mask) 학습 데이터 \n",
        "- mask_prob : 전체 토큰의 15% 정도\n",
        "- 15% 학습데이터 구성\n",
        " - [MASK] 토큰 : 80% (전체의 12%)\n",
        " - 랜덤한 토큰 : 10% (전체의 1.5%)\n",
        " - 원래의 토큰 : 10% (전체의 1.5%)\n",
        "\n",
        "- 띄어쓰기 단위로 한꺼번에 마스킹 : subword 로 나누어져있는 토큰을 단어단위로 모으는 작업\n",
        " - [ _ ]: u\"\\u2581\" : 단어의 시작에 대한 특수 문자\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfh6N1wQAC76",
        "outputId": "96250d92-11ef-45df-fce5-aa5532668a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('total token:', 97, 'mask token:', 14)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
        "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
        "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
        "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
        "print(tokens_org)\n",
        "\n",
        "# 전체 token의 15% mask\n",
        "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
        "'total token:', len(tokens_org), 'mask token:' , mask_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiEpKPTrAKai",
        "outputId": "d75038f6-8545-4705-d092-ee52edd0130b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
            "[5, 6] ['▁비', '가']\n",
            "[7, 8] ['▁내', '리는']\n",
            "[9, 10, 11] ['▁날', '이었', '어']\n",
            "[12, 13, 14] ['▁그', '날', '은']\n",
            "[15, 16, 17] ['▁', '왠', '지']\n",
            "[18, 19, 20] ['▁손', '님', '이']\n",
            "[21, 22] ['▁많', '아']\n",
            "[23] ['▁첫']\n",
            "[24, 25] ['▁번', '에']\n",
            "[26, 27] ['▁삼', '십']\n",
            "[28] ['▁전']\n",
            "[29, 30, 31] ['▁둘', '째', '번']\n",
            "[32, 33] ['▁오', '십']\n",
            "[34] ['▁전']\n",
            "[35, 36, 37] ['▁오', '랜', '만에']\n",
            "[38, 39, 40] ['▁받아', '보', '는']\n",
            "[41] ['▁십']\n",
            "[42, 43, 44] ['▁전', '짜', '리']\n",
            "[45, 46, 47] ['▁백', '통', '화']\n",
            "[48, 49, 50] ['▁서', '푼', '에']\n",
            "[52, 53, 54] ['▁손', '바', '닥']\n",
            "[55, 56] ['▁위', '엔']\n",
            "[57, 58, 59] ['▁기', '쁨', '의']\n",
            "[60, 61] ['▁눈', '물이']\n",
            "[62, 63] ['▁흘', '러']\n",
            "[64, 65, 66] ['▁컬', '컬', '한']\n",
            "[67, 68] ['▁목', '에']\n",
            "[69, 70] ['▁모', '주']\n",
            "[71, 72, 73] ['▁한', '잔', '을']\n",
            "[74, 75] ['▁적', '셔']\n",
            "[76] ['▁몇']\n",
            "[77] ['▁달']\n",
            "[78] ['▁포']\n",
            "[79, 80] ['▁전', '부터']\n",
            "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
            "[85] ['▁아내']\n",
            "[86, 87] ['▁생각', '에']\n",
            "[88, 89, 90] ['▁그', '토', '록']\n",
            "[91, 92] ['▁먹', '고']\n",
            "[93, 94, 95] ['▁싶', '다', '던']\n"
          ]
        }
      ],
      "source": [
        "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
        "cand_idx = []  # word 단위의 index array\n",
        "for (i, token) in enumerate(tokens_org):\n",
        "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "        continue\n",
        "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
        "        cand_idx[-1].append(i)\n",
        "    else:\n",
        "        cand_idx.append([i])\n",
        "\n",
        "# 결과확인\n",
        "for cand in cand_idx:\n",
        "    print(cand, [tokens_org[i] for i in cand])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUfhSBHrBi1J",
        "outputId": "53f6944b-1491-4648-e9eb-466e7ee96c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_org\n",
            "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
            "\n",
            "tokens\n",
            "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '[MASK]', '[MASK]', '[MASK]', '▁그', '날', '은', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '[MASK]', '[MASK]', '▁한', '잔', '을', '▁적', '셔', '▁몇', '[MASK]', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "# random mask를 위해서 index 순서를 섞음\n",
        "random.shuffle(cand_idx)\n",
        "\n",
        "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
        "tokens = copy.deepcopy(tokens_org)\n",
        "\n",
        "mask_lms = []  # mask 된 값\n",
        "for index_set in cand_idx:\n",
        "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
        "          break\n",
        "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
        "          continue\n",
        "    dice = random.random()  # 0과 1 사이의 확률 값\n",
        "\n",
        "    for index in index_set:\n",
        "        masked_token = None\n",
        "        if dice < 0.8:  # 80% replace with [MASK]\n",
        "            masked_token = \"[MASK]\"\n",
        "        elif dice < 0.9: # 10% keep original\n",
        "            masked_token = tokens[index]\n",
        "        else:  # 10% random word\n",
        "            masked_token = random.choice(vocab_list)\n",
        "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
        "        tokens[index] = masked_token\n",
        "\n",
        "print(\"tokens_org\")\n",
        "print(tokens_org, \"\\n\")\n",
        "print(\"tokens\")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxwb5z8EB3O4"
      },
      "source": [
        "### Masked LM의 라벨 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf7Mf8SpB4tA",
        "outputId": "d4aaca66-55b4-4ef4-e834-6481d7f4c7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask_idx   : [5, 6, 9, 10, 11, 15, 16, 17, 18, 19, 20, 69, 70, 77]\n",
            "mask_label : ['▁비', '가', '▁날', '이었', '어', '▁', '왠', '지', '▁손', '님', '이', '▁모', '주', '▁달']\n"
          ]
        }
      ],
      "source": [
        "# 순서 정렬 및 mask_idx, mask_label 생성\n",
        "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
        "mask_idx = [p[\"index\"] for p in mask_lms]\n",
        "mask_label = [p[\"label\"] for p in mask_lms]\n",
        "\n",
        "print(\"mask_idx   :\", mask_idx) # 15%에 해당하는 mask토큰\n",
        "print(\"mask_label :\", mask_label)\n",
        "\n",
        "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
        "    \"\"\"\n",
        "    마스크 생성\n",
        "    :param tokens: tokens\n",
        "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
        "    :param vocab_list: vocab list (random token 용)\n",
        "    :return tokens: mask된 tokens\n",
        "    :return mask_idx: mask된 token의 index\n",
        "    :return mask_label: mask된 token의 원래 값\n",
        "    \"\"\"\n",
        "    # 단어 단위로 mask 하기 위해서 index 분할\n",
        "    cand_idx = []  # word 단위의 index array\n",
        "    for (i, token) in enumerate(tokens):\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "            continue\n",
        "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
        "            cand_idx[-1].append(i)\n",
        "        else:\n",
        "            cand_idx.append([i])\n",
        "    # random mask를 위해서 순서를 섞음\n",
        "    random.shuffle(cand_idx)\n",
        "\n",
        "    mask_lms = []  # mask 된 값\n",
        "    for index_set in cand_idx:\n",
        "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
        "            break\n",
        "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
        "            continue\n",
        "        dice = random.random()  # 0..1 사이의 확률 값\n",
        "        for index in index_set:\n",
        "            masked_token = None\n",
        "            if dice < 0.8:  # 80% replace with [MASK]\n",
        "                masked_token = \"[MASK]\"\n",
        "            elif dice < 0.9: # 10% keep original\n",
        "                masked_token = tokens[index]\n",
        "            else:  # 10% random word\n",
        "                masked_token = random.choice(vocab_list)\n",
        "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
        "            tokens[index] = masked_token\n",
        "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
        "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
        "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
        "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
        "\n",
        "    return tokens, mask_idx, mask_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by8UhpB0B_GI"
      },
      "source": [
        "수행결과 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfttvILVCCQp",
        "outputId": "f448c61e-61a1-48ca-9fa4-9eb4271c7a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_org\n",
            "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
            "\n",
            "tokens\n",
            "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '[MASK]', '[MASK]', '[MASK]', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '[MASK]', '[MASK]', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '[MASK]', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '[MASK]', '[MASK]', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁들어가', '▁후손', '▁몇', '▁달', '▁포', '▁전', '부터', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
            "\n",
            "mask_idx   : [9, 10, 11, 26, 27, 41, 60, 61, 74, 75, 81, 82, 83, 84]\n",
            "mask_label : ['▁날', '이었', '어', '▁삼', '십', '▁십', '▁눈', '물이', '▁적', '셔', '▁콜', '록', '거', '리는']\n"
          ]
        }
      ],
      "source": [
        "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
        "tokens = copy.deepcopy(tokens_org)\n",
        "\n",
        "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
        "\n",
        "print(\"tokens_org\")\n",
        "print(tokens_org, \"\\n\")\n",
        "print(\"tokens\")\n",
        "print(tokens, \"\\n\")\n",
        "\n",
        "print(\"mask_idx   :\", mask_idx)\n",
        "print(\"mask_label :\", mask_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLS8us_rgR8r"
      },
      "source": [
        "\n",
        "# 3. 데이터 전처리 (2) NSP pair 생성\n",
        "NSP 데이터 구성\n",
        "- 두 문장이 연속하는지 확인하기 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 설정\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV6zOPW6CaJx",
        "outputId": "612e07fc-5601-4f26-974d-0fe313af8b5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
              " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
              " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "string = \"\"\"추적추적 비가 내리는 날이었어\n",
        "그날은 왠지 손님이 많아\n",
        "첫 번에 삼십 전 둘째 번 오십 전\n",
        "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
        "손바닥 위엔 기쁨의 눈물이 흘러\n",
        "컬컬한 목에 모주 한잔을 적셔\n",
        "몇 달 포 전부터 콜록거리는 아내\n",
        "생각에 그토록 먹고 싶다던\n",
        "설렁탕 한 그릇을 이제는 살 수 있어\n",
        "집으로 돌아가는 길 난 문득 떠올라\n",
        "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
        "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
        "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
        "아내의 간절한 목소리가 들려와\n",
        "나를 원망하듯 비는 점점 거세져\n",
        "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
        "난 몰라 오늘은 운수 좋은 날\n",
        "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\"\n",
        "\n",
        "# 줄 단위로 tokenize\n",
        "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
        "doc[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZLxN4EFCckr",
        "outputId": "68911d0a-b63a-4929-83fb-06cfcf057845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
            "tokens_a: 22 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아']\n",
            "tokens_b: 40 ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
            "\n",
            "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
            "tokens_a: 32 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던']\n",
            "tokens_b: 39 ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
            "\n",
            "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
            "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
            "tokens_b: 56 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
            "\n",
            "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
            "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
            "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 최대 길이\n",
        "n_test_seq = 64\n",
        "# 최소 길이\n",
        "min_seq = 8\n",
        "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
        "max_seq = n_test_seq - 3\n",
        "\n",
        "current_chunk = []  # line 단위 tokens\n",
        "current_length = 0\n",
        "for i in range(len(doc)):  # doc 전체를 loop\n",
        "    current_chunk.append(doc[i])  # line 단위로 추가\n",
        "    current_length += len(doc[i])  # current_chunk의 token 수\n",
        "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다. \n",
        "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
        "\n",
        "        #######################################\n",
        "        # token a\n",
        "        a_end = 1\n",
        "        if 1 < len(current_chunk):\n",
        "            a_end = random.randrange(1, len(current_chunk))\n",
        "        tokens_a = []\n",
        "        for j in range(a_end):\n",
        "            tokens_a.extend(current_chunk[j])\n",
        "        # token b\n",
        "        tokens_b = []\n",
        "        for j in range(a_end, len(current_chunk)):\n",
        "            tokens_b.extend(current_chunk[j])\n",
        "          \n",
        "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
        "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
        "        #######################################\n",
        "        print()\n",
        "\n",
        "        current_chunk = []\n",
        "        current_length = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4xzwbwhCo9h"
      },
      "source": [
        "### 문장 길이 조정\n",
        " - 문장A,B가 이어져있을때 두문장의 길이가 max_seq보다 길경우 : 두문장이 이어지는것이 유지되도록 A문장은 앞쪽을 trim, B문장은 뒤쪽을 trim처리한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ9cMOKYF1h3",
        "outputId": "451100d7-db4c-4120-fe09-05b81b8b3190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
            "is_next: 0\n",
            "tokens_a: 39 ['▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
            "tokens_b: 22 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아']\n",
            "\n",
            "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
            "is_next: 1\n",
            "tokens_a: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
            "tokens_b: 49 ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거']\n",
            "\n",
            "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
            "is_next: 0\n",
            "tokens_a: 44 ['▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
            "tokens_b: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
            "\n",
            "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
            "is_next: 1\n",
            "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
            "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
        "    \"\"\"\n",
        "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
        "    :param tokens_a: tokens A\n",
        "    :param tokens_b: tokens B\n",
        "    :param max_seq: 두 tokens 길이의 최대 값\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_seq:\n",
        "            break\n",
        "\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            del tokens_a[0]\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "current_chunk = []  # line 단위 tokens\n",
        "current_length = 0\n",
        "for i in range(len(doc)):  # doc 전체를 loop\n",
        "    current_chunk.append(doc[i])  # line 단위로 추가\n",
        "    current_length += len(doc[i])  # current_chunk의 token 수\n",
        "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
        "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
        "\n",
        "        # token a\n",
        "        a_end = 1\n",
        "        if 1 < len(current_chunk):\n",
        "            a_end = random.randrange(1, len(current_chunk))\n",
        "        tokens_a = []\n",
        "        for j in range(a_end):\n",
        "            tokens_a.extend(current_chunk[j])\n",
        "        # token b\n",
        "        tokens_b = []\n",
        "        for j in range(a_end, len(current_chunk)):\n",
        "            tokens_b.extend(current_chunk[j])\n",
        "\n",
        "        #######################################\n",
        "        if random.random() < 0.5:  # 50% 확률로 swap\n",
        "            is_next = 0     #False (뒤바뀐 문장)\n",
        "            tokens_t = tokens_a\n",
        "            tokens_a = tokens_b\n",
        "            tokens_b = tokens_t\n",
        "        else:\n",
        "            is_next = 1    #True (이어지는 문장)\n",
        "        # max_seq 보다 큰 경우 길이 조절\n",
        "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
        "        assert 0 < len(tokens_a)\n",
        "        assert 0 < len(tokens_b)\n",
        "\n",
        "        print(\"is_next:\", is_next)\n",
        "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
        "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
        "        #######################################\n",
        "        print()\n",
        "\n",
        "        current_chunk = []\n",
        "        current_length = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU0gyNH2F7it"
      },
      "source": [
        "### segmentation\n",
        "- 두 문장 사이에 segment 설정\n",
        " - 첫 번째 문장 segment : 0\n",
        " - 두 번째 문장 segment : 1 \n",
        " - 두문장사이 구분자 : [SEP] 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg4pC_MTGeSG",
        "outputId": "68316221-a86b-4f1d-e986-ac51b76b717b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
            "is_next: 0\n",
            "tokens_a: 39 ['▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
            "tokens_b: 22 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아']\n",
            "tokens: 64 ['[CLS]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]']\n",
            "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "masked tokens: 64 ['[CLS]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '[MASK]', '▁오', '십', '[MASK]', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '[MASK]', '[MASK]', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]']\n",
            "masked index: 9 [8, 11, 15, 16, 17, 31, 32, 38, 39]\n",
            "masked label: 9 ['▁번', '▁전', '▁받아', '보', '는', '▁위', '엔', '▁흘', '러']\n",
            "\n",
            "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
            "is_next: 1\n",
            "tokens_a: 45 ['▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라']\n",
            "tokens_b: 16 ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
            "tokens: 64 ['[CLS]', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]']\n",
            "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "masked tokens: 64 ['[CLS]', '▁적', '셔', '[MASK]', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '[MASK]', '[MASK]', '▁살', '▁수', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]']\n",
            "masked index: 9 [1, 2, 3, 30, 31, 35, 36, 37, 38]\n",
            "masked label: 9 ['▁적', '셔', '▁몇', '▁이', '제는', '▁집', '으로', '▁돌아', '가는']\n",
            "\n",
            "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
            "is_next: 0\n",
            "tokens_a: 31 ['와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
            "tokens_b: 30 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달']\n",
            "tokens: 64 ['[CLS]', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져', '[SEP]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '[SEP]']\n",
            "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "masked tokens: 64 ['[CLS]', '[MASK]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '[MASK]', '[MASK]', '[MASK]', '▁더', '해', '져', '[SEP]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '[MASK]', '▁옆', '에', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '[SEP]']\n",
            "masked index: 9 [1, 26, 27, 28, 43, 46, 47, 48, 49]\n",
            "masked label: 9 ['와', '▁', '걱', '정은', '▁내', '▁있어', '▁달', '라', '던']\n",
            "\n",
            "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
            "is_next: 0\n",
            "tokens_a: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
            "tokens_b: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
            "tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
            "segment: 25 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "masked tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
            "masked index: 3 [11, 12, 13]\n",
            "masked label: 3 ['▁좋', '을', '까']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "instances = []\n",
        "current_chunk = []  # line 단위 tokens\n",
        "current_length = 0\n",
        "for i in range(len(doc)):  # doc 전체를 loop\n",
        "    current_chunk.append(doc[i])  # line 단위로 추가\n",
        "    current_length += len(doc[i])  # current_chunk의 token 수\n",
        "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
        "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
        "\n",
        "        # token a\n",
        "        a_end = 1\n",
        "        if 1 < len(current_chunk):\n",
        "            a_end = random.randrange(1, len(current_chunk))\n",
        "        tokens_a = []\n",
        "        for j in range(a_end):\n",
        "            tokens_a.extend(current_chunk[j])\n",
        "        # token b\n",
        "        tokens_b = []\n",
        "        for j in range(a_end, len(current_chunk)):\n",
        "            tokens_b.extend(current_chunk[j])\n",
        "\n",
        "        if random.random() < 0.5:  # 50% 확률로 swap\n",
        "            is_next = 0    # False\n",
        "            tokens_t = tokens_a\n",
        "            tokens_a = tokens_b\n",
        "            tokens_b = tokens_t\n",
        "        else:\n",
        "            is_next = 1   # True\n",
        "        # max_seq 보다 큰 경우 길이 조절\n",
        "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
        "        assert 0 < len(tokens_a)\n",
        "        assert 0 < len(tokens_b)\n",
        "\n",
        "        print(\"is_next:\", is_next)\n",
        "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
        "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
        "        #######################################\n",
        "\n",
        "        # tokens & segment 생성\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "        print(\"tokens:\", len(tokens), tokens)\n",
        "        print(\"segment:\", len(segment), segment)\n",
        "        \n",
        "        # mask\n",
        "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
        "        print(\"masked tokens:\", len(tokens), tokens)\n",
        "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
        "        print(\"masked label:\", len(mask_label), mask_label)\n",
        "\n",
        "        instance = {\n",
        "            \"tokens\": tokens,\n",
        "            \"segment\": segment,\n",
        "            \"is_next\": is_next,\n",
        "            \"mask_idx\": mask_idx,\n",
        "            \"mask_label\": mask_label\n",
        "        }\n",
        "        instances.append(instance)\n",
        "        #######################################\n",
        "        print()\n",
        "\n",
        "        current_chunk = []\n",
        "        current_length = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnWZ7PhIGf6C",
        "outputId": "1d7049d4-4a0c-4991-ed48-0edfdb3e966b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tokens': ['[CLS]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '[MASK]', '▁오', '십', '[MASK]', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '[MASK]', '[MASK]', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [8, 11, 15, 16, 17, 31, 32, 38, 39], 'mask_label': ['▁번', '▁전', '▁받아', '보', '는', '▁위', '엔', '▁흘', '러']}\n",
            "{'tokens': ['[CLS]', '▁적', '셔', '[MASK]', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '[MASK]', '[MASK]', '▁살', '▁수', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 30, 31, 35, 36, 37, 38], 'mask_label': ['▁적', '셔', '▁몇', '▁이', '제는', '▁집', '으로', '▁돌아', '가는']}\n",
            "{'tokens': ['[CLS]', '[MASK]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '[MASK]', '[MASK]', '[MASK]', '▁더', '해', '져', '[SEP]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '[MASK]', '▁옆', '에', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 26, 27, 28, 43, 46, 47, 48, 49], 'mask_label': ['와', '▁', '걱', '정은', '▁내', '▁있어', '▁달', '라', '던']}\n",
            "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [11, 12, 13], 'mask_label': ['▁좋', '을', '까']}\n"
          ]
        }
      ],
      "source": [
        "# 최종 데이터셋 결과 확인\n",
        "for instance in instances:\n",
        "    print(instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-snKGRxGn2U"
      },
      "source": [
        "### 코퍼스 생성 메소드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGiftA9GqzF",
        "outputId": "e6257e7a-ae7d-4231-901a-e63c212e4eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tokens': ['[CLS]', '▁오', '랜', '만에', '▁받아', '보', '는', '[MASK]', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁계속', '떡', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '[MASK]', '▁오', '십', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 34, 35, 41, 42, 43, 53, 54, 60], 'mask_label': ['▁십', '▁비', '가', '▁그', '날', '은', '▁번', '에', '▁번']}\n",
            "{'tokens': ['[CLS]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '[MASK]', '[MASK]', '[MASK]', '▁적', '셔', '▁몇', '[MASK]', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '[MASK]', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 31, 39, 40, 41], 'mask_label': ['▁모', '주', '▁한', '잔', '을', '▁달', '▁아내', '▁생각', '에']}\n",
            "{'tokens': ['[CLS]', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '[MASK]', '[MASK]', '[MASK]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁나', '가지', '▁말', '라', '던', '[MASK]', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 13, 14, 46, 47, 48, 49, 50, 56], 'mask_label': ['▁들', '려', '와', '▁오늘', '은', '▁', '왠', '지', '▁내']}\n",
            "{'tokens': ['[CLS]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '[MASK]', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [21, 22, 23], 'mask_label': ['▁좋', '을', '까']}\n"
          ]
        }
      ],
      "source": [
        "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
        "    \"\"\"\n",
        "    doc별 pretrain 데이터 생성\n",
        "    \"\"\"\n",
        "    # for CLS], [SEP], [SEP]\n",
        "    max_seq = n_seq - 3\n",
        "\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for i in range(len(doc)):\n",
        "        current_chunk.append(doc[i])  # line\n",
        "        current_length += len(doc[i])\n",
        "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
        "            # token a\n",
        "            a_end = 1\n",
        "            if 1 < len(current_chunk):\n",
        "                a_end = random.randrange(1, len(current_chunk))\n",
        "            tokens_a = []\n",
        "            for j in range(a_end):\n",
        "                tokens_a.extend(current_chunk[j])\n",
        "            # token b\n",
        "            tokens_b = []\n",
        "            for j in range(a_end, len(current_chunk)):\n",
        "                tokens_b.extend(current_chunk[j])\n",
        "\n",
        "            if random.random() < 0.5:  # 50% 확률로 swap\n",
        "                is_next = 0\n",
        "                tokens_t = tokens_a\n",
        "                tokens_a = tokens_b\n",
        "                tokens_b = tokens_t\n",
        "            else:\n",
        "                is_next = 1\n",
        "            # max_seq 보다 큰 경우 길이 조절\n",
        "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
        "            assert 0 < len(tokens_a)\n",
        "            assert 0 < len(tokens_b)\n",
        "            # tokens & aegment 생성\n",
        "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "            # mask\n",
        "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
        "\n",
        "            instance = {\n",
        "                \"tokens\": tokens,\n",
        "                \"segment\": segment,\n",
        "                \"is_next\": is_next,\n",
        "                \"mask_idx\": mask_idx,\n",
        "                \"mask_label\": mask_label\n",
        "            }\n",
        "            instances.append(instance)\n",
        "\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "    return instances\n",
        "\n",
        "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
        "\n",
        "# 최종 데이터셋 결과 확인\n",
        "for instance in instances:\n",
        "    print(instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KvtEXGFgR00"
      },
      "source": [
        "# 4. 데이터 전처리 (3) 데이터셋 완성\n",
        "- make_pretrain_data 함수\n",
        " - BERT pretrain 데이터셋을 생성 \n",
        " - json 포맷으로 저장(bert_pre_train.json)\n",
        "\n",
        "- load_pre_train_data 함수\n",
        " - 학습에 필요한 데이터를 로딩하는 함수\n",
        " - np.memmap을 사용해 메모리 사용량을 최소화하여 로딩\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4d9345947e7d47a68154ced90d089b48",
            "06ebef6046364f938cd7fe0b73c8dbf9",
            "66933ab0ae944089b1eff7e673da98a1",
            "44344f5b40b6417183a28ec9e02b2e28",
            "e059f23e452d4dc9944bade075c5ab0e",
            "7a4858e9f2364914a02f57e7c88b6066",
            "7dbb8877da2245bd8d805e4d91d202c2",
            "0a408dd8bf7b4475ba8d6bf55faee49e",
            "f510eee3ec1f4ce7b9b16b5f587525fd",
            "9e263766cee549429cd29b83716b8798",
            "68553c948b6e453bad8305d3bb99c313"
          ]
        },
        "id": "hHHH11xwHEId",
        "outputId": "d8b71e83-e370-4ae1-850d-2fbd3a63dfa8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3957761 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d9345947e7d47a68154ced90d089b48"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "corpus_file = '/content/data/kowiki.txt'\n",
        "\n",
        "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
        "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
        "    def save_pretrain_instances(out_f, doc):\n",
        "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
        "        for instance in instances:\n",
        "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
        "            out_f.write(\"\\n\")\n",
        "\n",
        "    # 특수문자 7개를 제외한 vocab_list 생성\n",
        "    vocab_list = []\n",
        "    for id in range(7, len(vocab)):\n",
        "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
        "            vocab_list.append(vocab.id_to_piece(id))\n",
        "\n",
        "    # line count 확인\n",
        "    line_cnt = 0\n",
        "    with open(in_file, \"r\") as in_f:\n",
        "        for line in in_f:\n",
        "            line_cnt += 1\n",
        "\n",
        "    with open(in_file, \"r\") as in_f:\n",
        "        with open(out_file, \"w\") as out_f:\n",
        "            doc = []\n",
        "            for line in tqdm(in_f, total=line_cnt):\n",
        "                line = line.strip()\n",
        "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
        "                    if 0 < len(doc):\n",
        "                        save_pretrain_instances(out_f, doc)\n",
        "                        doc = []\n",
        "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
        "                    pieces = vocab.encode_as_pieces(line)\n",
        "                    if 0 < len(pieces):\n",
        "                        doc.append(pieces)\n",
        "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
        "                save_pretrain_instances(out_f, doc)\n",
        "                doc = []\n",
        "\n",
        "pretrain_json_path = 'data/bert_pre_train.json'\n",
        "\n",
        "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PO-xy25zHiT7"
      },
      "outputs": [],
      "source": [
        "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
        "    \"\"\"\n",
        "    학습에 필요한 데이터를 로드\n",
        "    :param vocab: vocab\n",
        "    :param filename: 전처리된 json 파일\n",
        "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
        "    :param count: 데이터 수 제한 (None이면 전체)\n",
        "    :return enc_tokens: encoder inputs\n",
        "    :return segments: segment inputs\n",
        "    :return labels_nsp: nsp labels\n",
        "    :return labels_mlm: mlm labels\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            total += 1\n",
        "            # 데이터 수 제한\n",
        "            if count is not None and count <= total:\n",
        "                break\n",
        "    \n",
        "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
        "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
        "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        for i, line in enumerate(tqdm(f, total=total)):\n",
        "            if total <= i:\n",
        "                print(\"data load early stop\", total, i)\n",
        "                break\n",
        "            data = json.loads(line)\n",
        "            # encoder token\n",
        "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
        "            enc_token += [0] * (n_seq - len(enc_token))\n",
        "            # segment\n",
        "            segment = data[\"segment\"]\n",
        "            segment += [0] * (n_seq - len(segment))\n",
        "            # nsp label\n",
        "            label_nsp = data[\"is_next\"]\n",
        "            # mlm label\n",
        "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
        "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
        "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
        "            label_mlm[mask_idx] = mask_label\n",
        "\n",
        "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
        "\n",
        "            enc_tokens[i] = enc_token\n",
        "            segments[i] = segment\n",
        "            labels_nsp[i] = label_nsp\n",
        "            labels_mlm[i] = label_mlm\n",
        "\n",
        "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d6649b5c0db468bba812b345e9392b6",
            "61e8b89cd17c40fd9580f699b4f7516d",
            "0065be9c4eb24d0ea21538e3b68aac8b",
            "8be85c05b4d341ac8ed7d80e3ffd9e22",
            "2d898d5906ec418ba8c85191e613b6bb",
            "27f4b52342db4e96b73df37fcfd2bba9",
            "90838039a99848cd81b4d6bee69b2d3d",
            "806058aa898f4eb2be1a07f9a20efaf5",
            "6f94d898541b45818924d6aaf2c25573",
            "4298fb6112b54e5ca6f567491d51c207",
            "339fad0a69224a55bc71e8106f28514c"
          ]
        },
        "id": "03fIf2FyIzTU",
        "outputId": "3025d61b-b618-4f8b-d15f-a8e36f52c6b5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/128000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d6649b5c0db468bba812b345e9392b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data load early stop 128000 128000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(memmap([   5,    6, 1605, 3599, 1755, 3630,   41, 3644,  830, 3624,    6,\n",
              "            6,    6,   13,   81,   87, 1501,    6,   25, 3779, 3873, 3667,\n",
              "         3631, 3813, 3873, 4196, 3636, 3779, 3601,  249, 3725, 1232,   33,\n",
              "           52, 3599,  479, 3652, 3625,  243, 2780,   14, 1509,    6,    6,\n",
              "            6,  165, 1697, 4290, 3873, 3703, 3683,    6,   21, 5007,    6,\n",
              "         1927, 3607,  813,   17, 3599, 3746,  587,  931,  103, 4313, 4290,\n",
              "          613, 3638, 3718,   98, 3878, 3656,  256, 2543,  309,  337, 3735,\n",
              "          181, 3616, 3603,    6,    6,    6,    4,   18, 3686,  207, 3714,\n",
              "         3324, 1042,  103, 3610, 3686, 3718,  207, 3714,   37, 3418,  416,\n",
              "          810, 3666, 3625,  131, 3662,    7, 3629,  203,  241, 3602, 1114,\n",
              "         3724,  788,    6,   49, 3632,  796,  663, 1647, 3682, 3682, 3625,\n",
              "          203, 3008, 3625, 3616,   16, 3599,    4], dtype=int32),\n",
              " memmap([   5,   13,   81, 3604,    6,    6, 6313, 3562, 4500,    6,  316,\n",
              "         1425,  173,  305, 3620, 1395,  149, 3607,   19,    6, 3596, 4904,\n",
              "         3750, 3603, 4065,  115, 3617, 3756, 3596, 4639, 1364, 3627,  991,\n",
              "         3616, 3600,    7, 3614, 3746,    9, 2972,  173, 1345, 3604,  848,\n",
              "         3784, 3833,    8, 3637, 2263,   12, 3614, 3746,  836, 3596, 4904,\n",
              "         3750, 3603, 4065,  115, 3600, 2972,  173,  351, 3599,    4,  848,\n",
              "         3784, 3833,    8, 3637, 3676,  848, 3784, 1931,   58, 3676,  416,\n",
              "         2316, 3619, 3625, 3617, 3744, 4335,   12, 3625, 3616,  175, 3662,\n",
              "            7, 3629,  203,  578, 3652, 3625, 3617, 4148, 3665,  143, 3625,\n",
              "         3616,  131, 3662,  342, 3629, 3616, 3602,    6,    6,    6, 1115,\n",
              "         3665, 1381, 4148, 3451, 1633,  375,  671, 1644, 3608,  547, 3423,\n",
              "            6,    6,    6,  752, 3608, 3604,    4], dtype=int32),\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
              " 0,\n",
              " 0,\n",
              " memmap([   0,   10,    0,    0,    0,    0,    0,    0,    0,    0, 1135,\n",
              "           52, 3599,    0,    0,    0,    0, 2247,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,  168, 3877,\n",
              "          414,    0,    0,    0,    0,    0,    0,  593,    0,    0,  399,\n",
              "            0,    0,    0,    0,    0,  307,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,  489,  376, 3599,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,   37, 3418,  416,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,  243,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
              " memmap([   0,    0,    0,    0,   15, 3784,   68, 3238, 3602,   13,    0,\n",
              "         1425,  173,    0,    0,    0,    0,    0,    0,  805,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,   58, 3676,  416,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,  176,  334,  829,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          765,  815, 3604,    0,    0,    0,    0], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 128000건만 메모리에 로딩\n",
        "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)\n",
        "\n",
        "# 처음과 마지막 확인\n",
        "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeKJGjQKgRs0"
      },
      "source": [
        "# 5. BERT 모델 구현\n",
        "\n",
        "\n",
        "Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLCPm6VrKI5Y"
      },
      "source": [
        "### 유틸리티 함수\n",
        "- pad mask 함수\n",
        "- ahead mask 함수\n",
        "- gelu activation 함수\n",
        "- parameter initializer 생성 함수, \n",
        "- json을 config 형태설정 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "55wfF5QYKKQH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def get_pad_mask(tokens, i_pad=0):\n",
        "    \"\"\"\n",
        "    pad mask 계산하는 함수\n",
        "    :param tokens: tokens (bs, n_seq)\n",
        "    :param i_pad: id of pad\n",
        "    :return mask: pad mask (pad: 1, other: 0)\n",
        "    \"\"\"\n",
        "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
        "    mask = tf.expand_dims(mask, axis=1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_ahead_mask(tokens, i_pad=0):\n",
        "    \"\"\"\n",
        "    ahead mask 계산하는 함수\n",
        "    :param tokens: tokens (bs, n_seq)\n",
        "    :param i_pad: id of pad\n",
        "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
        "    \"\"\"\n",
        "    n_seq = tf.shape(tokens)[1]\n",
        "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
        "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
        "    pad_mask = get_pad_mask(tokens, i_pad)\n",
        "    mask = tf.maximum(ahead_mask, pad_mask)\n",
        "    return mask\n",
        "\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    gelu activation 함수\n",
        "    :param x: 입력 값\n",
        "    :return: gelu activation result\n",
        "    \"\"\"\n",
        "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
        "\n",
        "def kernel_initializer(stddev=0.02):\n",
        "    \"\"\"\n",
        "    parameter initializer 생성\n",
        "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
        "    \"\"\"\n",
        "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
        "\n",
        "\n",
        "def bias_initializer():\n",
        "    \"\"\"\n",
        "    bias initializer 생성\n",
        "    \"\"\"\n",
        "    return tf.zeros_initializer\n",
        "\n",
        "\n",
        "class Config(dict):\n",
        "    \"\"\"\n",
        "    json을 config 형태로 사용하기 위한 Class\n",
        "    :param dict: config dictionary\n",
        "    \"\"\"\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        \"\"\"\n",
        "        file에서 Config를 생성 함\n",
        "        :param file: filename\n",
        "        \"\"\"\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCo0Nx_xKKIr"
      },
      "source": [
        "### embedding 레이어\n",
        "- Token Embedding\n",
        "- Segment Embedding\n",
        "- Position Embedding : 별도의 레이어를 구현하지 않고 BERT 클래스에서 간단히 포함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1if-sUheKKBA",
        "outputId": "57282184-3cdb-4032-96a9-b50f7dfb3964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "class SharedEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Weighed Shaed Embedding Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.n_vocab = config.n_vocab\n",
        "        self.d_model = config.d_model\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        shared weight 생성\n",
        "        :param input_shape: Tensor Shape (not used)\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"shared_embedding_weight\"):\n",
        "            self.shared_weights = self.add_weight(\n",
        "                \"weights\",\n",
        "                shape=[self.n_vocab, self.d_model],\n",
        "                initializer=kernel_initializer()\n",
        "            )\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\"):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: 입력\n",
        "        :param mode: 실행 모드\n",
        "        :return: embedding or linear 실행 결과\n",
        "        \"\"\"\n",
        "        # mode가 embedding일 경우 embedding lookup 실행\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs)\n",
        "        # mode가 linear일 경우 linear 실행\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        # mode가 기타일 경우 오류 발생\n",
        "        else:\n",
        "            raise ValueError(f\"mode {mode} is not valid.\")\n",
        "    \n",
        "    def _embedding(self, inputs):\n",
        "        \"\"\"\n",
        "        embedding lookup\n",
        "        :param inputs: 입력\n",
        "        \"\"\"\n",
        "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
        "        return embed\n",
        "\n",
        "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
        "        \"\"\"\n",
        "        linear 실행\n",
        "        :param inputs: 입력\n",
        "        \"\"\"\n",
        "        n_batch = tf.shape(inputs)[0]\n",
        "        n_seq = tf.shape(inputs)[1]\n",
        "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
        "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
        "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PositionEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Position Embedding Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"position_embedding\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: 입력\n",
        "        :return embed: position embedding lookup 결과\n",
        "        \"\"\"\n",
        "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
        "        embed = self.embedding(position)\n",
        "        return embed\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMx7zbpzKJ3Q"
      },
      "source": [
        "### Attention\n",
        " - ScaleDotProductAttention\n",
        " - MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pW2dK_ZaKJbx"
      },
      "outputs": [],
      "source": [
        "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Scale Dot Product Attention Class\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param Q: Q value\n",
        "        :param K: K value\n",
        "        :param V: V value\n",
        "        :param attn_mask: 실행 모드\n",
        "        :return attn_out: attention 실행 결과\n",
        "        \"\"\"\n",
        "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
        "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
        "        attn_scale = tf.math.divide(attn_score, scale)\n",
        "        attn_scale -= 1.e9 * attn_mask\n",
        "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
        "        attn_out = tf.matmul(attn_prob, V)\n",
        "        return attn_out\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi Head Attention Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"multi_head_attention\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.d_model = config.d_model\n",
        "        self.n_head = config.n_head\n",
        "        self.d_head = config.d_head\n",
        "\n",
        "        # Q, K, V input dense layer\n",
        "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        # Scale Dot Product Attention class\n",
        "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
        "        # output dense layer\n",
        "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "\n",
        "    def call(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param Q: Q value\n",
        "        :param K: K value\n",
        "        :param V: V value\n",
        "        :param attn_mask: 실행 모드\n",
        "        :return attn_out: attention 실행 결과\n",
        "        \"\"\"\n",
        "        # reshape Q, K, V, attn_mask\n",
        "        batch_size = tf.shape(Q)[0]\n",
        "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
        "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
        "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
        "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
        "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
        "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
        "        # transpose and liner\n",
        "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
        "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
        "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
        "\n",
        "        return attn_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KEiyxG9LdoI"
      },
      "source": [
        "- FeedForward 레이어\n",
        "- transformer encoder 레이어\n",
        "- BERT 레이어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "camswm1PLuh3"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Position Wise Feed Forward Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"feed_forward\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: inputs\n",
        "        :return ff_val: feed forward 실행 결과\n",
        "        \"\"\"\n",
        "        ff_val = self.W_2(self.W_1(inputs))\n",
        "        return ff_val\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Encoder Layer Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"encoder_layer\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(config)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForward(config)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
        " \n",
        "    def call(self, enc_embed, self_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
        "        :param self_mask: enc_tokens의 pad mask\n",
        "        :return enc_out: EncoderLayer 실행 결과\n",
        "        \"\"\"\n",
        "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
        "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
        "\n",
        "        ffn_val = self.ffn(norm1_val)\n",
        "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
        "\n",
        "        return enc_out\n",
        "\n",
        "\n",
        "class BERT(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    BERT Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"bert\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.i_pad = config.i_pad\n",
        "        self.embedding = SharedEmbedding(config)\n",
        "        self.position = PositionEmbedding(config)\n",
        "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
        "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "        \n",
        "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: (enc_tokens, segments)\n",
        "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
        "        \"\"\"\n",
        "        enc_tokens, segments = inputs\n",
        "\n",
        "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
        "\n",
        "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
        "\n",
        "        enc_out = self.dropout(enc_embed)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
        "\n",
        "        logits_cls = enc_out[:,0]\n",
        "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
        "        return logits_cls, logits_lm\n",
        "    \n",
        "    def get_embedding(self, tokens, segments):\n",
        "        \"\"\"\n",
        "        token embedding, position embedding lookup\n",
        "        :param tokens: 입력 tokens\n",
        "        :param segments: 입력 segments\n",
        "        :return embed: embedding 결과\n",
        "        \"\"\"\n",
        "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
        "        embed = self.norm(embed)\n",
        "        return embed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-yQy_GOMfnS"
      },
      "source": [
        "아주 작은 pretrain용 BERT 모델(test_model)을 생성하여 동작을 미리 확인해보겠다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DO91-JKzL12t"
      },
      "outputs": [],
      "source": [
        "# Encoder Layer class 정의\n",
        "class PooledOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        " \n",
        "    def call(self, inputs):\n",
        "        outputs = self.dense1(inputs)\n",
        "        outputs = self.dense2(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def build_model_pre_train(config):\n",
        "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
        "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
        "\n",
        "    bert = BERT(config)\n",
        "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
        "\n",
        "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
        "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
        "\n",
        "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
        "\n",
        "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyFTwKY-gRjG"
      },
      "source": [
        "# 6. pretrain 진행\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAohZ1mSTHDk"
      },
      "source": [
        "### loss, accuracy 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X2T0eJquMi4-"
      },
      "outputs": [],
      "source": [
        "def lm_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    loss 계산 함수\n",
        "    :param y_true: 정답 (bs, n_seq)\n",
        "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
        "    \"\"\"\n",
        "    # loss 계산\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
        "    # pad(0) 인 부분 mask\n",
        "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
        "\n",
        "def lm_acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    acc 계산 함수\n",
        "    :param y_true: 정답 (bs, n_seq)\n",
        "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
        "    \"\"\"\n",
        "    # 정답 여부 확인\n",
        "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
        "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
        "    # pad(0) 인 부분 mask\n",
        "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
        "    matches *= mask\n",
        "    # 정확도 계산\n",
        "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wZGdDykTMfH"
      },
      "source": [
        "### Learning Rate 스케줄링\n",
        "- WarmUp 이후 consine 형태로 감소하는 스케줄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "v6d7XQveTRfg",
        "outputId": "399a77f1-f7e2-41d2-b8f1-bff3d037da44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    CosineSchedule Class\n",
        "    \"\"\"\n",
        "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param train_steps: 학습 step 총 합\n",
        "        :param warmup_steps: warmup steps\n",
        "        :param max_lr: 최대 learning rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert 0 < warmup_steps < train_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.train_steps = train_steps\n",
        "        self.max_lr = max_lr\n",
        "\n",
        "    def __call__(self, step_num):\n",
        "        \"\"\"\n",
        "        learning rate 계산\n",
        "        :param step_num: 현재 step number\n",
        "        :retrun: 계산된 learning rate\n",
        "        \"\"\"\n",
        "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
        "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
        "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
        "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
        "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
        "\n",
        "# compute lr \n",
        "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
        "lrs = []\n",
        "for step_num in range(4000):\n",
        "    lrs.append(test_schedule(float(step_num)).numpy())\n",
        "\n",
        "# draw\n",
        "plt.plot(lrs, 'r-', label='learning_rate')\n",
        "plt.xlabel('Step')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRz6klIaTcAS"
      },
      "source": [
        "### 모델 생성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHg2IZvSdIGP"
      },
      "source": [
        "파라미터 사이즈 : 약 1M (1,025,080)개\n",
        "\n",
        "- batch_size : 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config({\"d_model\": 100, \"n_head\": 4, \"d_head\": 32, \"dropout\": 0.1, \"d_ff\": 256, \n",
        "                 \"layernorm_epsilon\": 0.001, \"n_layer\": 2, \"n_seq\": 64, \"n_vocab\": 0, \"i_pad\": 0})\n",
        "\n",
        "config.n_vocab = len(vocab)\n",
        "config.i_pad = vocab.pad_id()\n",
        "\n",
        "pre_train_model = build_model_pre_train(config)\n",
        "pre_train_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VUaGl5DdtO3",
        "outputId": "5df24b1d-ea39-40ba-9ea0-8967ba3e9a17"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " enc_tokens (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " segments (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " bert (BERT)                    ((None, 100),        1014780     ['enc_tokens[0][0]',             \n",
            "                                 (None, None, 8007)               'segments[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pooled_nsp (PooledOutput)      (None, 2)            10300       ['bert[0][0]']                   \n",
            "                                                                                                  \n",
            " nsp (Softmax)                  (None, 2)            0           ['pooled_nsp[0][0]']             \n",
            "                                                                                                  \n",
            " mlm (Softmax)                  (None, None, 8007)   0           ['bert[0][1]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,025,080\n",
            "Trainable params: 1,025,080\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32 \n",
        "\n",
        "# optimizer\n",
        "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
        "print(\"train_steps:\", train_steps)\n",
        "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# compile\n",
        "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXTv2FyidvYP",
        "outputId": "607a2b0b-08e4-43ca-dfd6-b0647c4b8513"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_steps: 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save weights callback\n",
        "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
        "# train\n",
        "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fMl9FUBJY--",
        "outputId": "1d1b7602-ea56-45a2-e095-74596a59e455"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 16.9385 - nsp_loss: 0.6007 - mlm_loss: 16.3378 - nsp_acc: 0.6648 - mlm_lm_acc: 0.1326\n",
            "Epoch 1: mlm_lm_acc improved from -inf to 0.13260, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 146s 36ms/step - loss: 16.9382 - nsp_loss: 0.6007 - mlm_loss: 16.3376 - nsp_acc: 0.6648 - mlm_lm_acc: 0.1326\n",
            "Epoch 2/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 16.8944 - nsp_loss: 0.6067 - mlm_loss: 16.2877 - nsp_acc: 0.6540 - mlm_lm_acc: 0.1333\n",
            "Epoch 2: mlm_lm_acc improved from 0.13260 to 0.13333, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 144s 36ms/step - loss: 16.8944 - nsp_loss: 0.6067 - mlm_loss: 16.2877 - nsp_acc: 0.6540 - mlm_lm_acc: 0.1333\n",
            "Epoch 3/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 16.6938 - nsp_loss: 0.6060 - mlm_loss: 16.0877 - nsp_acc: 0.6556 - mlm_lm_acc: 0.1357\n",
            "Epoch 3: mlm_lm_acc improved from 0.13333 to 0.13573, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 144s 36ms/step - loss: 16.6934 - nsp_loss: 0.6060 - mlm_loss: 16.0874 - nsp_acc: 0.6556 - mlm_lm_acc: 0.1357\n",
            "Epoch 4/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 16.4178 - nsp_loss: 0.6043 - mlm_loss: 15.8134 - nsp_acc: 0.6583 - mlm_lm_acc: 0.1405\n",
            "Epoch 4: mlm_lm_acc improved from 0.13573 to 0.14048, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 145s 36ms/step - loss: 16.4174 - nsp_loss: 0.6043 - mlm_loss: 15.8131 - nsp_acc: 0.6583 - mlm_lm_acc: 0.1405\n",
            "Epoch 5/10\n",
            "4000/4000 [==============================] - ETA: 0s - loss: 15.9835 - nsp_loss: 0.6034 - mlm_loss: 15.3801 - nsp_acc: 0.6607 - mlm_lm_acc: 0.1472\n",
            "Epoch 5: mlm_lm_acc improved from 0.14048 to 0.14717, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 144s 36ms/step - loss: 15.9835 - nsp_loss: 0.6034 - mlm_loss: 15.3801 - nsp_acc: 0.6607 - mlm_lm_acc: 0.1472\n",
            "Epoch 6/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 15.7300 - nsp_loss: 0.6000 - mlm_loss: 15.1299 - nsp_acc: 0.6679 - mlm_lm_acc: 0.1516\n",
            "Epoch 6: mlm_lm_acc improved from 0.14717 to 0.15159, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 144s 36ms/step - loss: 15.7300 - nsp_loss: 0.6001 - mlm_loss: 15.1300 - nsp_acc: 0.6679 - mlm_lm_acc: 0.1516\n",
            "Epoch 7/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 15.5901 - nsp_loss: 0.5967 - mlm_loss: 14.9935 - nsp_acc: 0.6729 - mlm_lm_acc: 0.1546\n",
            "Epoch 7: mlm_lm_acc improved from 0.15159 to 0.15462, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 144s 36ms/step - loss: 15.5900 - nsp_loss: 0.5967 - mlm_loss: 14.9933 - nsp_acc: 0.6729 - mlm_lm_acc: 0.1546\n",
            "Epoch 8/10\n",
            "4000/4000 [==============================] - ETA: 0s - loss: 15.5015 - nsp_loss: 0.5930 - mlm_loss: 14.9085 - nsp_acc: 0.6786 - mlm_lm_acc: 0.1563\n",
            "Epoch 8: mlm_lm_acc improved from 0.15462 to 0.15632, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 146s 36ms/step - loss: 15.5015 - nsp_loss: 0.5930 - mlm_loss: 14.9085 - nsp_acc: 0.6786 - mlm_lm_acc: 0.1563\n",
            "Epoch 9/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 15.4475 - nsp_loss: 0.5900 - mlm_loss: 14.8575 - nsp_acc: 0.6850 - mlm_lm_acc: 0.1575\n",
            "Epoch 9: mlm_lm_acc improved from 0.15632 to 0.15754, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 149s 37ms/step - loss: 15.4476 - nsp_loss: 0.5900 - mlm_loss: 14.8577 - nsp_acc: 0.6850 - mlm_lm_acc: 0.1575\n",
            "Epoch 10/10\n",
            "3999/4000 [============================>.] - ETA: 0s - loss: 15.4214 - nsp_loss: 0.5887 - mlm_loss: 14.8327 - nsp_acc: 0.6866 - mlm_lm_acc: 0.1578\n",
            "Epoch 10: mlm_lm_acc improved from 0.15754 to 0.15779, saving model to /content/models/bert_pre_train.hdf5\n",
            "4000/4000 [==============================] - 145s 36ms/step - loss: 15.4217 - nsp_loss: 0.5887 - mlm_loss: 14.8330 - nsp_acc: 0.6866 - mlm_lm_acc: 0.1578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training result\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
        "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
        "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rmyhUaJBd8oV",
        "outputId": "1a37dd08-8de9-4779-f959-2e2986333e40"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 정리.\n",
        "\n",
        "\n",
        "**mini Bert 모델 성능 비교**\n",
        "\n",
        "| 학습시 설정 사항 |10M 모델\t| 1M 모델(v4_최종)| 1M 모델(v1)| \n",
        "|---|---|---| ---|\n",
        "|에폭 | 10회  | 10회 |10회 |\n",
        "| 모델 파라미터| 약 천만개( 10,695,936) | **약 백만개(1,025,080)** |약 125만개(1,249,536) |\n",
        "| 배치사이즈| 64 |**32**  |64|\n",
        "| n_vocab| 32007| 8007|8007|\n",
        "|d_ff | 1024|256|1024|\n",
        "| d_head| 64|32|64|\n",
        "| d_model|256 |**100**|64|\n",
        "| n_head| 4|4|4|\n",
        "| n_layer| 3|2|3|\n",
        "| n_seq|256 |64|256|\n",
        "|---|---|---|---|\n",
        "|loss | 13.76|15.43|16.94|\n",
        "|nsp_loss | 0.44|0.58|0.60|\n",
        "|mlm_loss | 13.32|14.84|16.34|\n",
        "| nsp_acc|0.87 |0.68|0.66|\n",
        "| mlm_lm_acc|0.24 |0.16|0.13|\n",
        "\n",
        "\n",
        "- 이전 LMS 노드에서 만들었던 10M Bert모델의 경우 파라미터 튜닝없이  설정된 대로 학습을 진행한 결과이다.\n",
        "-  학습 파라미터가 5M개 정도일때 기존 성능보다 loss가 12이하로 줄기도 하지만 본 프로젝트에서는 1M로 모델 파라미터를 줄인 bert모델로 성능을 확인해보았다. config의 파라미터를 튜닝해 로스와 정확도를 비슷한 수준으로 만들기위해 파라미터 튜닝을 진행하였다.\n",
        " - 파라미터를 10배로 줄여서인지 로스를 크게 줄이긴 어려웠지만 파라미터 튜닝으로 loss및 정확도에서 유의미한 성능 향상을 가져올수 있었다.\n",
        " "
      ],
      "metadata": {
        "id": "UXu6-Xsa65F7"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP14_Assignment_MiniBERT_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d9345947e7d47a68154ced90d089b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06ebef6046364f938cd7fe0b73c8dbf9",
              "IPY_MODEL_66933ab0ae944089b1eff7e673da98a1",
              "IPY_MODEL_44344f5b40b6417183a28ec9e02b2e28"
            ],
            "layout": "IPY_MODEL_e059f23e452d4dc9944bade075c5ab0e"
          }
        },
        "06ebef6046364f938cd7fe0b73c8dbf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4858e9f2364914a02f57e7c88b6066",
            "placeholder": "​",
            "style": "IPY_MODEL_7dbb8877da2245bd8d805e4d91d202c2",
            "value": "100%"
          }
        },
        "66933ab0ae944089b1eff7e673da98a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a408dd8bf7b4475ba8d6bf55faee49e",
            "max": 3957761,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f510eee3ec1f4ce7b9b16b5f587525fd",
            "value": 3957761
          }
        },
        "44344f5b40b6417183a28ec9e02b2e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e263766cee549429cd29b83716b8798",
            "placeholder": "​",
            "style": "IPY_MODEL_68553c948b6e453bad8305d3bb99c313",
            "value": " 3957761/3957761 [05:54&lt;00:00, 15501.00it/s]"
          }
        },
        "e059f23e452d4dc9944bade075c5ab0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4858e9f2364914a02f57e7c88b6066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbb8877da2245bd8d805e4d91d202c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a408dd8bf7b4475ba8d6bf55faee49e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f510eee3ec1f4ce7b9b16b5f587525fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e263766cee549429cd29b83716b8798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68553c948b6e453bad8305d3bb99c313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d6649b5c0db468bba812b345e9392b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61e8b89cd17c40fd9580f699b4f7516d",
              "IPY_MODEL_0065be9c4eb24d0ea21538e3b68aac8b",
              "IPY_MODEL_8be85c05b4d341ac8ed7d80e3ffd9e22"
            ],
            "layout": "IPY_MODEL_2d898d5906ec418ba8c85191e613b6bb"
          }
        },
        "61e8b89cd17c40fd9580f699b4f7516d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27f4b52342db4e96b73df37fcfd2bba9",
            "placeholder": "​",
            "style": "IPY_MODEL_90838039a99848cd81b4d6bee69b2d3d",
            "value": "100%"
          }
        },
        "0065be9c4eb24d0ea21538e3b68aac8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_806058aa898f4eb2be1a07f9a20efaf5",
            "max": 128000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f94d898541b45818924d6aaf2c25573",
            "value": 128000
          }
        },
        "8be85c05b4d341ac8ed7d80e3ffd9e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4298fb6112b54e5ca6f567491d51c207",
            "placeholder": "​",
            "style": "IPY_MODEL_339fad0a69224a55bc71e8106f28514c",
            "value": " 128000/128000 [00:40&lt;00:00, 4497.50it/s]"
          }
        },
        "2d898d5906ec418ba8c85191e613b6bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27f4b52342db4e96b73df37fcfd2bba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90838039a99848cd81b4d6bee69b2d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "806058aa898f4eb2be1a07f9a20efaf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f94d898541b45818924d6aaf2c25573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4298fb6112b54e5ca6f567491d51c207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "339fad0a69224a55bc71e8106f28514c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}