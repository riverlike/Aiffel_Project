{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP16_Assignment_HuggingFaceMNLI.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace 커스텀 프로젝트 직접 만들기\n",
        "\n",
        "이번 프로젝트에서는 Huggingface 프레임워크를 사용해 GLUE Dataset의 MNLI task를 진행해보겠다. "
      ],
      "metadata": {
        "id": "zdpudVXGcUvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GLUE dataset : MNLI(The Multi-Genre Natural Language Inference) Corpus\n",
        " - 참고 : [GLUE Paper](https://openreview.net/pdf?id=rJ4km2R5t7)\n",
        " - 이전 LMS에서 살펴보았던 MRPC 코퍼스와 MNLI 코퍼스를 비교해보면 아래와 같다.\n",
        " \n",
        "|Corpus | Train | Test | Task | Metrics | Domain | \n",
        "|---|---|---|---|---|---|\n",
        "| MNLI| 393k|20k | NLI|matched acc./mismatched acc. |  misc.|\n",
        "| MRPC | 3.7k|1.7k | paraphrase|acc./F1  | news|\n",
        "\n",
        "- MNLI Task : 여러 장르에 대한 문장을 3가지로 분류하는 자연어 추론 과제이다.\n",
        "  - 두 개 문장이 참(entailment), 거짓(contradiction), 중립 혹은 판단불가(neutral)인지 가려내는 것\n",
        "  - 예) 나 출근했어 + 난 백수야 → 거짓(contradiction)\n",
        "    "
      ],
      "metadata": {
        "id": "wGBRSiqF_3bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelzoo-client[transformers]"
      ],
      "metadata": {
        "id": "Rezn0WboJ6sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoConfig\n",
        "from dataclasses import asdict\n",
        "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n"
      ],
      "metadata": {
        "id": "MevXcvwnKrmU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 1. mnli 데이터셋을 분석해 보기\n",
        "tensorflow-datasets를 이용하여 glue/mnli를 다운로드하려면 tensorflow-datasets 라이브러리 버전을 올려야 한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cEuucv8Pe5PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets -U"
      ],
      "metadata": {
        "id": "bK3Lo0EpJ0dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, info = tfds.load('glue/mnli', with_info=True)\n",
        "info.splits['train'].num_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEWMnwuyLCFJ",
        "outputId": "599564c7-889b-4e52-ef40-8dcede959329"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392702"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "392702개의 샘플이 있다.\n",
        "\n",
        "\n",
        "|Corpus | Train | Test | Task | Metrics | Domain | \n",
        "|---|---|---|---|---|---|\n",
        "| MNLI| 393k|20k | NLI|matched acc./mismatched acc. |  misc.|\n",
        "\n",
        "앞서 살펴보았던 train dataset의 명세 처럼 393k에 해당하는 훈련 샘플이 있는것을 확인 할수 있다. 데이터셋 안에 어떤 항목이 있는지 확인해보자."
      ],
      "metadata": {
        "id": "xVm6pCX0LQ8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'].take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd5KUa74LM2h",
        "outputId": "0c95bee4-179f-400d-c769-dd6edbf1174f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNLI corpus에는 2가지 case에 대한 validation, test셋이 존재한다. \n",
        "본 프로젝트에서는 matched한 case에 대해서만 학습을 진행할것이다.\n",
        "\n",
        "- validation_matched, validation_mismatched\n",
        "- test_matched, test_mismatched\n",
        "\n"
      ],
      "metadata": {
        "id": "vXel_Lb_Y95C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWX_pCPkY9di",
        "outputId": "7543c6e0-6c60-4570-8805-07848686ada7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_matched': <PrefetchDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>,\n",
              " 'test_mismatched': <PrefetchDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>,\n",
              " 'train': <PrefetchDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>,\n",
              " 'validation_matched': <PrefetchDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>,\n",
              " 'validation_mismatched': <PrefetchDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['test_matched'].take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMbYFl8gYvb-",
        "outputId": "af02ac6b-2de6-4565-d7c3-b29fe3a556ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec={'hypothesis': TensorSpec(shape=(), dtype=tf.string, name=None), 'idx': TensorSpec(shape=(), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'premise': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "안의 내용도 함께 살펴보자."
      ],
      "metadata": {
        "id": "LsOKK3FyLdlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = data['train'].take(1)\n",
        "for example in examples:    \n",
        "    hypothesis = example['hypothesis']    \n",
        "    premise = example['premise']\n",
        "    label = example['label']\n",
        "    print(hypothesis)\n",
        "    print(premise)\n",
        "    print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W58AY6bLd-H",
        "outputId": "1ee34ac7-adea-4638-bdfa-34d891b17040"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Meaningful partnerships with stakeholders is crucial.', shape=(), dtype=string)\n",
            "tf.Tensor(b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', shape=(), dtype=string)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = data['validation_matched'].take(2)\n",
        "for example in examples:    \n",
        "    hypothesis = example['hypothesis']    \n",
        "    premise = example['premise']\n",
        "    label = example['label']\n",
        "    print(hypothesis)\n",
        "    print(premise)\n",
        "    print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZrs8bqNZRjK",
        "outputId": "7edafa8b-03f9-4135-f0a4-b64ce383f08b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'yeah lots of people for the right life ', shape=(), dtype=string)\n",
            "tf.Tensor(b'uh-huh oh yeah all the people for right uh life or something', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(b' I will be assuming that the 6.0a cost of the Postal Service to take the mail from basic to workshared condition is constant.', shape=(), dtype=string)\n",
            "tf.Tensor(b'Also, I will be assuming that the 6.0a cost of the Postal Service to take the mail from basic to workshared condition is constant as limited quantities of mail move back and forth between basic and workshared.', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2. MNLIProcessor클래스 구현하기\n",
        "\n",
        "Processor 기능\n",
        "- Raw Dataset를 Annotated Dataset으로 변환\n",
        "- 항목별로 text_a, text_b, label 등의 annotation이 포함된 InputExample로 변환함.\n"
      ],
      "metadata": {
        "id": "dk8AQTOye5Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_example_from_tensor_dict(self, tensor_dict):\n",
        "        \"\"\"\n",
        "        Gets an example from a dict with tensorflow tensors.\n",
        "\n",
        "        Args:\n",
        "            tensor_dict: Keys and values should match the corresponding Glue\n",
        "                tensorflow_dataset examples.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def tfds_map(self, example):\n",
        "        \"\"\"\n",
        "        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts\n",
        "        examples to the correct format.\n",
        "        \"\"\"\n",
        "        if len(self.get_labels()) > 1:\n",
        "            example.label = self.get_labels()[int(example.label)]\n",
        "        return example\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
        "\n",
        "\n",
        "class MNLIProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MNLI data set (GLUE version).\"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def get_example_from_tensor_dict(self, tensor_dict):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return InputExample(\n",
        "            tensor_dict[\"idx\"].numpy(),\n",
        "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
        "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
        "            str(tensor_dict[\"label\"].numpy()),\n",
        "        )\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\", \"2\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[3]\n",
        "            text_b = line[4]\n",
        "            label = None if set_type == \"test\" else line[0]\n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n"
      ],
      "metadata": {
        "id": "Fj1-uXbYKifp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " MNLIProcessor의 get_example_from_tensor_dict 함수로 전처리된 데이터를 확인해보자."
      ],
      "metadata": {
        "id": "kKEmFx2URKax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = MNLIProcessor()\n",
        "examples = data['train'].take(3)\n",
        "\n",
        "for example in examples:\n",
        "    print('------원본데이터------')\n",
        "    print(example)  \n",
        "    example = processor.get_example_from_tensor_dict(example)\n",
        "    print('------processor 가공데이터------')\n",
        "    print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nRC2DScQxNq",
        "outputId": "259a4bdf-0548-43ca-cd13-a191b4e6fd01"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------원본데이터------\n",
            "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'Meaningful partnerships with stakeholders is crucial.'>, 'idx': <tf.Tensor: shape=(), dtype=int32, numpy=16399>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.'>}\n",
            "------processor 가공데이터------\n",
            "InputExample(guid=16399, text_a='Meaningful partnerships with stakeholders is crucial.', text_b='In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', label='1')\n",
            "------원본데이터------\n",
            "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'The Clinton followers kept to the higher ground in the discussion. '>, 'idx': <tf.Tensor: shape=(), dtype=int32, numpy=206287>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b'The Clinton surrogates also held the high ground in the context war.'>}\n",
            "------processor 가공데이터------\n",
            "InputExample(guid=206287, text_a='The Clinton followers kept to the higher ground in the discussion. ', text_b='The Clinton surrogates also held the high ground in the context war.', label='0')\n",
            "------원본데이터------\n",
            "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'Women have jobs in all areas of the workforce, they are almost getting the same wages as most men,'>, 'idx': <tf.Tensor: shape=(), dtype=int32, numpy=352707>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b\"um-hum because women are in every field now i mean i can't think of a field that they're not involved in\">}\n",
            "------processor 가공데이터------\n",
            "InputExample(guid=352707, text_a='Women have jobs in all areas of the workforce, they are almost getting the same wages as most men,', text_b=\"um-hum because women are in every field now i mean i can't think of a field that they're not involved in\", label='1')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3. 데이터셋 구성하기\n",
        "- 위에서 구현한 processor 및 Huggingface에서 제공하는 tokenizer를 활용하여 데이터셋 구성해보자.\n",
        "\n",
        "- mnli task에서는 **bert-base-cased** 모델을 사용해볼 것이다.\n",
        " - [참고](https://huggingface.co/gchhablani/bert-base-cased-finetuned-mnli)\n"
      ],
      "metadata": {
        "id": "YHF7BQxee49W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLWGaQtMKi0h",
        "outputId": "bddd5778-0073-4723-d00a-ff83fff04b11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf_glue_convert_examples_to_features 함수\n",
        "- 최종적으로 모델에 전달될 tf.data.Dataset 인스턴스를 생성"
      ],
      "metadata": {
        "id": "GQisuf_pT7Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"claasification\") :\n",
        "    if max_length is None :\n",
        "        max_length = tokenizer.max_len\n",
        "    if label_list is None:\n",
        "        label_list = processor.get_labels()\n",
        "        print(\"Using label list %s\" % (label_list))\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "    labels = [label_map[example.label] for example in examples]\n",
        "\n",
        "    batch_encoding = tokenizer(\n",
        "        [(example.text_a, example.text_b) for example in examples],\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "\n",
        "        feature = InputFeatures(**inputs, label=labels[i])\n",
        "        features.append(feature)\n",
        "\n",
        "    for i, example in enumerate(examples[:1]):\n",
        "        print(\"*** Example ***\")\n",
        "        print(\"guid: %s\" % (example.guid))\n",
        "        print(\"features: %s\" % features[i])\n",
        "\n",
        "    return features\n",
        "\n",
        "def tf_glue_convert_examples_to_features(examples, tokenizer, max_length, processor, label_list=None, output_mode=\"classification\") :\n",
        "    \"\"\"\n",
        "    :param examples: tf.data.Dataset\n",
        "    :param tokenizer: pretrained tokenizer\n",
        "    :param max_length: example의 최대 길이(기본값 : tokenizer의 max_len)\n",
        "    :param task: GLUE task 이름\n",
        "    :param label_list: 라벨 리스트\n",
        "    :param output_mode: \"regression\" or \"classification\"\n",
        "\n",
        "    :return: task에 맞도록 feature가 구성된 tf.data.Dataset\n",
        "    \"\"\"\n",
        "    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n",
        "    features = _glue_convert_examples_to_features(examples, tokenizer, max_length, processor)\n",
        "    label_type = tf.int64\n",
        "\n",
        "    def gen():\n",
        "        for ex in features:\n",
        "            d = {k: v for k, v in asdict(ex).items() if v is not None}\n",
        "            label = d.pop(\"label\")\n",
        "            yield (d, label)\n",
        "\n",
        "    input_names = [\"input_ids\"] + tokenizer.model_input_names\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({k: tf.int32 for k in input_names}, label_type),\n",
        "        ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "aEDjjuqqTJRx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 데이터셋\n",
        "train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, processor=processor)\n",
        "train_dataset_batch = train_dataset.shuffle(100).batch(16).repeat(2)"
      ],
      "metadata": {
        "id": "x4GdmM-zWKri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation 데이터셋\n",
        "validation_dataset = tf_glue_convert_examples_to_features(data['validation_matched'], tokenizer, max_length=128, processor=processor)\n",
        "validation_dataset_batch = validation_dataset.shuffle(100).batch(16)"
      ],
      "metadata": {
        "id": "mEWBr7KnWL1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터셋\n",
        "test_dataset = tf_glue_convert_examples_to_features(data['test_matched'], tokenizer, max_length=128, processor=processor)\n",
        "test_dataset_batch = test_dataset.shuffle(100).batch(16)"
      ],
      "metadata": {
        "id": "nOfX5zrKTwHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = train_dataset.take(1)\n",
        "for example in examples:\n",
        "    print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xue1Jhs4UETQ",
        "outputId": "ccc725a7-375f-4bfe-f225-297d4d81c744"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
            "array([  101, 15902, 13797,  2007, 22859,  2003, 10232,  1012,   102,\n",
            "        1999,  5038,  1997,  2122, 13136,  1010,  1048, 11020,  2038,\n",
            "        2499, 29454, 29206, 14626,  2144,  2786,  2000, 16636,  1996,\n",
            "       10908,  1997,  1996,  2110,  4041,  6349,  1998,  2000,  5323,\n",
            "       15902, 13797,  2007, 22859,  6461,  2012,  6469,  2075,  1037,\n",
            "        2047, 25353, 14905, 10735,  2483,  2090,  1996,  2976, 10802,\n",
            "        1998, 15991,  1997,  3423,  2578,  4804,  1012,   102,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
            "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4. model을 생성하여 학습 및 테스트를 진행해 보기\n"
      ],
      "metadata": {
        "id": "Gl4YwJrre4gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사전훈련된 BERT 모델로 학습을 진행해보겠다."
      ],
      "metadata": {
        "id": "bpFY4_TRUpce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(processor.get_labels())\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SQ-xsaRKjHY",
        "outputId": "22d636d4-f709-449d-c60c-eefe448fa3b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, epochs=2, steps_per_epoch=115, validation_data=validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "T6aJswUNcPiB",
        "outputId": "a5c92099-2dc3-4302-b58e-711ba8af5583"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6650d1641180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m115\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 1000, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 212, in __call__\n        batch_dim = tf.shape(y_t)[0]\n\n    ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.evaluate(test_dataset)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "LUSJShuVcaIW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정리.\n",
        "\n",
        "HuggingFace 프레임워크를 사용해 2문장 사이의 자연어 추론 task를 진행해보았다. 그동안 Going Deeper 프로젝트를 진행하며 무수히 많은 오류를 뱉어내던 전처리, 토크나이징의 과정이 한두줄의 함수로 간단히 해결이 되었다!!\n",
        "\n",
        "코드를 깊이 이해하는 시간을 가지지 못해 학습이 완료되지 못했지만 HuggingFace 프레임워크의  전체적인 구조와 흐름을 이해하게 되었다. 빠르게 개발해야 할 task들이 있다면 HuggingFace와 같은 프레임워크들을 활용하는것도 모델을 개발하는 것만큼이나 중요할 것이다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mItxM5J6vGd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference\n",
        "- [GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS\n",
        "PLATFORM](https://openreview.net/pdf?id=rJ4km2R5t7)"
      ],
      "metadata": {
        "id": "z6SiOcOOhwsX"
      }
    }
  ]
}